{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3154556d",
   "metadata": {},
   "source": [
    "# Detecting Gender Bias in English-German Translations using Natural Language Processing\n",
    "\n",
    "This notebook presents the bias detection model used for the demonstration. Minimal documentation and interpretation are provided, as it is intended to be read alongside the thesis.  \n",
    "\n",
    "The notebook performs the following steps:  \n",
    "1. Reads the created dataset.  \n",
    "2. Loads and trains a multilingual BERT model.  \n",
    "3. Evaluates the model on the held-out dataset and the handcrafted dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834f30",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Standard Python libraries are imported for data handling, computation, and plotting. PyTorch and the Transformers library are used for model training and inference. Evaluation metrics and dataset splitting functions are imported from scikit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# torch and dataset utils\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# transformers library\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# evaluation and data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e222663",
   "metadata": {},
   "source": [
    "## Set Seed\n",
    "A fixed seed is set for Python, NumPy, and PyTorch random number generators. This ensures that results are reproducible across runs. CUDA deterministic settings are enabled to maintain consistent GPU computations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                  \n",
    "    np.random.seed(seed)              \n",
    "    torch.manual_seed(seed)           \n",
    "    torch.cuda.manual_seed(seed)      \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6a176",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from a CSV file. This dataset was created using the `join_datasets.py` script located in the `/datasets` directory. It contains sentence pairs with labels indicating whether the translation exhibits gender bias. The first few rows are displayed for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a751c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/dataset.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f54aa9",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "\n",
    "The computational device is determined, prioritizing GPU if available. Device information is printed to confirm the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print device info\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU: None (using CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc0e30",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "The tokenizer corresponding to the selected mBERT model is loaded. This tokenizer is used to convert input text into the token IDs required by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba47b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_path = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e99331",
   "metadata": {},
   "source": [
    "## Load Model and Send to Device\n",
    "The pre-trained mBERT model is loaded with a sequence classification head configured for binary classification. The model is transferred to the previously selected device for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"neutral\", 1: \"biased\"},\n",
    "    label2id={\"neutral\": 0, \"biased\": 1}\n",
    ")\n",
    "\n",
    "# move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb879bcf",
   "metadata": {},
   "source": [
    "## Freeze Encoder Layers Count Trainable Parameters\n",
    "The first eight encoder layers (layers 0 to 7) of BERT are frozen to reduce training time while allowing higher layers to adapt to the task. Freezing fewer layers increases trainable parameters and flexibility, which was tested in additional experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze encoder layers 0 to 7\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"bert.encoder.layer.\"):\n",
    "        layer_num = int(name.split(\".\")[3])\n",
    "        if layer_num < 8:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# count and print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00540c44",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "The EN-DE dataset is prepared for training, validation, and testing. Minimal preprocessing is applied, as the data has already been cleaned and labeled for bias detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84745505",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "A custom `BiasDataset` class is defined to handle EN-DE sentence pairs. Each sample is tokenized using the mBERT tokenizer, with the German translation provided as the `text_pair`. The label tensor is included for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset for bias detection\n",
    "class BiasDataset(Dataset):\n",
    "    # init with dataframe and tokenizer\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # return number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # return one encoded sample\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.data.iloc[idx][\"english\"]\n",
    "        german = self.data.iloc[idx][\"german\"]\n",
    "        label = int(self.data.iloc[idx][\"label\"])\n",
    "\n",
    "        # tokenize EN-DE sentence pair\n",
    "        encoded = self.tokenizer(\n",
    "            text=english,\n",
    "            text_pair=german,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # encoded outputs have extra batch dimension, remove it with squeeze(0) to get plain tensors\n",
    "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
    "        # add label tensor to the dict under key 'labels'\n",
    "        item[\"labels\"] = torch.tensor(label)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc28df7",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "The dataset is split into train (80%), validation (10%), and test (10%) sets. Stratified sampling is applied to maintain the label distribution across all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e171490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train (80%) and temp (20%), keeping label distribution same with stratify\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# split temp into validation (10%) and test (10%) sets, stratified by label\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06d805",
   "metadata": {},
   "source": [
    "## Create Dataset Objects\n",
    "`BiasDataset` objects are created for the train, validation, and test sets. These objects provide tokenized EN-DE sentence pairs and labels to the model during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validation and test datasets\n",
    "train_dataset = BiasDataset(train_df, tokenizer)  \n",
    "val_dataset = BiasDataset(val_df, tokenizer)    \n",
    "test_dataset = BiasDataset(test_df, tokenizer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d441df9",
   "metadata": {},
   "source": [
    "# Training\n",
    "The mBERT model is trained on the EN-DE bias dataset. The training procedure follows the hyperparameters and setup described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be9511",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "Hyperparameters are defined for learning rate, batch size, number of epochs, and output directory. TrainingArguments are configured to perform evaluation, logging, and model saving at the end of each epoch. The best model is loaded automatically based on macro F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-5\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "\n",
    "output_dir=\"./model_output\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    seed = seed,\n",
    "    output_dir=output_dir,       \n",
    "    num_train_epochs=num_epochs,   \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    learning_rate=lr,             \n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",       \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"eval_f1_macro\",  \n",
    "    greater_is_better=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69782b",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Calculation for Classification\n",
    "A custom metric function is defined to calculate precision, recall, F1 score, and support for each class. Macro averages and overall accuracy are also computed to allow consistent evaluation across training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049aa80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        # per-class\n",
    "        **{f\"precision_class_{i}\": precision[i] for i in range(len(precision))},\n",
    "        **{f\"recall_class_{i}\":    recall[i]    for i in range(len(recall))},\n",
    "        **{f\"f1_class_{i}\":        f1[i]        for i in range(len(f1))},\n",
    "        **{f\"support_class_{i}\":   support[i]   for i in range(len(support))},\n",
    "        # overall\n",
    "        \"precision_macro\": np.mean(precision),\n",
    "        \"recall_macro\":    np.mean(recall),\n",
    "        \"f1_macro\":        np.mean(f1),\n",
    "        \"accuracy\":        (predictions == labels).mean(),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c91741",
   "metadata": {},
   "source": [
    "## Run trainer\n",
    "A `Trainer` object is instantiated with the model, datasets, training arguments, metric function, and early stopping callback. Training is executed with exception handling. After completion, the trained model and tokenizer are saved to the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "try:\n",
    "    train_results = trainer.train()\n",
    "except Exception as e:\n",
    "    print(\"Training failed:\", e)\n",
    "    raise\n",
    "\n",
    "print(\"Training complete. Saving model...\")\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e28c",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The trained mBERT model is evaluated on the validation and test sets. Both overall performance metrics and detailed error analyses are computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02efd18",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Confusion Rates\n",
    "\n",
    "A function is defined to compute confusion matrix components and false positive/negative rates. The `evaluate_and_print` function performs evaluation on a given dataset, prints the macro F1 score, confusion matrix, error rates, and a detailed classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7253a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rates(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    return tn, fp, fn, tp, fp_rate, fn_rate\n",
    "\n",
    "def evaluate_and_print(trainer, dataset, name):\n",
    "    print(f\"\\nEvaluating on {name} set...\")\n",
    "\n",
    "    # Evaluate with compute_metrics\n",
    "    results = trainer.evaluate(eval_dataset=dataset)\n",
    "    print(f\"{name} F1:\", round(results[\"eval_f1_macro\"], 3))\n",
    "\n",
    "    # Get raw predictions and labels\n",
    "    output = trainer.predict(dataset)\n",
    "    logits = output.predictions\n",
    "    labels = output.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Confusion matrix breakdown\n",
    "    tn, fp, fn, tp, fp_rate, fn_rate = compute_rates(labels, preds)\n",
    "    print(f\"{name} Confusion Matrix:\")\n",
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "    print(f\"False Positive Rate={fp_rate:.3f}, False Negative Rate={fn_rate:.3f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\n{name} Classification Report:\\n\",\n",
    "          classification_report(labels, preds, zero_division=0, digits=4))\n",
    "\n",
    "    return output  # Optional, in case you want to reuse preds later\n",
    "\n",
    "# ---- Main Evaluation ----\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "val_output  = evaluate_and_print(trainer, val_dataset,  \"Validation\")\n",
    "test_output = evaluate_and_print(trainer, test_dataset, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2fafd",
   "metadata": {},
   "source": [
    "## Detailed Error Analysis\n",
    "\n",
    "Predictions on the test set are combined with the corresponding EN-DE sentence pairs and true labels. False positives and false negatives are identified to allow inspection of model errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_en = test_dataset.data[\"english\"].tolist()\n",
    "test_texts_de = test_dataset.data[\"german\"].tolist()\n",
    "\n",
    "test_labels = test_output.label_ids\n",
    "test_preds  = np.argmax(test_output.predictions, axis=1)\n",
    "\n",
    "analysis_df = pd.DataFrame({\n",
    "    \"text_en\":    test_texts_en,\n",
    "    \"text_de\":    test_texts_de,\n",
    "    \"true_label\": test_labels,\n",
    "    \"pred_label\": test_preds\n",
    "})\n",
    "\n",
    "fp_df = analysis_df[(analysis_df.true_label == 0) & (analysis_df.pred_label == 1)]\n",
    "fn_df = analysis_df[(analysis_df.true_label == 1) & (analysis_df.pred_label == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a65bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "combined_df = pd.concat([\n",
    "    fp_df.assign(Error='False Positive'),\n",
    "    fn_df.assign(Error='False Negative')\n",
    "], ignore_index=True)[['Error', 'text_en', 'text_de']]\n",
    "\n",
    "combined_df.columns = ['Error Type', 'English Text', 'German Text']\n",
    "\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d08fb",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "A confusion matrix is visualized with both counts and percentages. This provides an overview of the model's performance on the test set, highlighting misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c576f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_counts_percent(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "    labels = ['Negative (0)', 'Positive (1)']\n",
    "\n",
    "    annot = [[f\"{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)\" for j in range(cm.shape[1])] for i in range(cm.shape[0])]\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Reds',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix_counts_percent(test_labels, test_preds, title=\"Test Set Confusion Matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d97164",
   "metadata": {},
   "source": [
    "# Test cases\n",
    "The trained mBERT model is evaluated on handcrafted test sentences. These sentences cover various bias patterns and scenarios, allowing inspection of the model's behavior beyond the standard test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911a70a",
   "metadata": {},
   "source": [
    "## Load trained model\n",
    "The model and tokenizer are loaded from the saved output directory and set to evaluation mode. This ensures that inference is performed without gradient updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model_output\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a459cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten text helper function\n",
    "def shorten(text, max_len=40):\n",
    "    if len(text) <= max_len:\n",
    "        return text\n",
    "    return text[:max_len-3] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8b85d",
   "metadata": {},
   "source": [
    "## Handcrafted Test Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeba3b5",
   "metadata": {},
   "source": [
    "A set of EN-DE sentence pairs is defined. Each pair is labeled as:  \n",
    "- 0 = neutral (no gender bias)  \n",
    "- 1 = biased (translation contains gender bias)  \n",
    "\n",
    "The test cases include neutral sentences, neutral sentences with gendered subjects, stereotypically biased sentences, gender-fair language (GFL) examples, and job posting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # neutral\n",
    "    (\"Today the weather is beautiful\", \"Heute ist das Wetter schön\", 0), \n",
    "    (\"I like this guy.\", \"Ich mag diesen Kerl.\", 0),\n",
    "    (\"How are you?\", \"Wie geht es dir?\", 0),\n",
    "    (\"BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion.\",\"BERT ist ein Transformer-Modell, das auf selbstüberwachte Weise auf einem großen Korpus mehrsprachiger Daten vorab trainiert wird.\",0),\n",
    "    (\"Knowledge is love and light and vision.\",\"Wissen ist Liebe und Licht und Vision.\",0),\n",
    "\n",
    "    # neutral with gendered subjects\n",
    "    (\"My mother is an engineer.\", \"Meine Mutter ist Ingenieurin.\", 0),\n",
    "    (\"Is this boy a good artist?\", \"Ist dieser Junge ein guter Künstler?\", 0),\n",
    "    (\"I am living with my sister, who is also my best friend\", \"Ich lebe mit meiner Schwester, die auch meine beste Freundin ist\", 0),\n",
    "    (\"My father was an excellent cook.\", \"Mein Vater war ein ausgezeichneter Koch.\", 0),\n",
    "    (\"The girls went hiking.\", \"Die Mädchen gingen wandern.\", 0),\n",
    "\n",
    "\n",
    "    # biased\n",
    "    (\"Do you like our maths teacher?\", \"Mögen Sie unsere Mathelehrerin?\", 1),\n",
    "    (\"The doctor was late to work today.\", \"Der Arzt kam heute zu spät zur Arbeit.\", 1),\n",
    "    (\"Tomorrow the students are leaving for a class trip.\", \"Morgen gehen die Studenten zu einer Klassenfahrt.\", 1),\n",
    "    (\"This nurse does not work hard.\", \"Diese Krankenschwester arbeitet nicht hart.\", 1),\n",
    "    (\"Athletes earn a lot of money.\", \"Sportler verdienen viel Geld.\", 1),\n",
    "\n",
    "    # test GFL\n",
    "    (\"Engineers are kind and hard-working\", \"Ingenieur*innen sind lieb und fleißig\", 0),\n",
    "    (\"Engineers are kind and hard-working\", \"Ingenieure und Ingenieurinnen sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrende sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrer:innen sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrerinnen und Lehrer sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrer sind lieb und fleißig\", 1),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrerinnen sind lieb und fleißig\", 1),\n",
    "\n",
    "    # job posting morgan stanley\n",
    "    (\"We’re seeking someone to join our team Office 365 squads to lead the design, development, and integration of Gen AI apps and integration using Microsoft Copilot Studio.\",\"Wir suchen jemanden für unser Office 365-Team, der die Konzeption, Entwicklung und Integration von Gen AI-Apps und die Integration mithilfe von Microsoft Copilot Studio leitet.\",0),\n",
    "    (\"The ideal candidate should have a solid technical foundation with a focus on Custom agent development and Copilot integrations, strategic thinking, excellent communication skills, and the ability to collaborate within a global team.\", \"Der ideale Kandidat sollte über solide technische Grundlagen mit Schwerpunkt auf der Entwicklung kundenspezifischer Agenten und Copilot-Integrationen, strategisches Denken, ausgezeichnete Kommunikationsfähigkeiten und die Fähigkeit zur Zusammenarbeit in einem globalen Team verfügen.\", 1),\n",
    "    (\"In the Technology division, we leverage innovation to build the connections and capabilities that power our Firm, enabling our clients and colleagues to redefine markets and shape the future of our communities.\", \"Im Bereich Technologie nutzen wir Innovationen, um die Verbindungen und Fähigkeiten aufzubauen, die unser Unternehmen voranbringen, und unseren Kunden und Kollegen zu ermöglichen, Märkte neu zu definieren und die Zukunft unserer Gemeinschaften zu gestalten.\",1),\n",
    "    (\"This is a Lead Workplace Engineering position at VP level, which is part of the job family responsible for managing and optimizing the technical environment and end-user experience across various workplace technologies, ensuring seamless operations and user satisfaction across the organization.\",\"Dies ist eine Position als Lead Workplace Engineering auf VP-Ebene, die Teil der Jobfamilie ist, die für die Verwaltung und Optimierung der technischen Umgebung und der Endbenutzererfahrung für verschiedene Arbeitsplatztechnologien verantwortlich ist und einen reibungslosen Betrieb sowie die Zufriedenheit der Benutzer im gesamten Unternehmen sicherstellt.\",1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f3dc8",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "The handcrafted sentences are converted into a `BiasDataset` instance. This allows tokenization and structured input to the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of test cases into a dataframe\n",
    "test_df = pd.DataFrame(test_cases, columns=[\"english\", \"german\", \"label\"])\n",
    "\n",
    "# create BiasDataset instance from dataframe\n",
    "test_dataset = BiasDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86608a33",
   "metadata": {},
   "source": [
    "## Run Inference on Each Test Case\n",
    "\n",
    "Each handcrafted sentence is passed through mBERT. Predictions, probabilities, and correctness indicators are collected for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14badba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    item = test_dataset[i]\n",
    "    \n",
    "    # prepare inputs for model, add batch dimension and move to device\n",
    "    inputs = {key: val.unsqueeze(0).to(device) for key, val in item.items() if key != \"labels\"}\n",
    "    \n",
    "    # run model in evaluation mode without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_label = torch.argmax(logits, dim=1).item()\n",
    "        prob = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # collect results\n",
    "    results.append({\n",
    "        \"english\": test_df.iloc[i][\"english\"],\n",
    "        \"german\": test_df.iloc[i][\"german\"],\n",
    "        \"true_label\": test_df.iloc[i][\"label\"],\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"neutral_prob\": prob[0],\n",
    "        \"biased_prob\": prob[1],\n",
    "        \"correct\": test_df.iloc[i][\"label\"] == pred_label\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0369a8",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "Results are summarized in a table showing English and German texts, true and predicted labels, probabilities for each class, and correctness. Overall model accuracy on the handcrafted test cases is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nBias detection test results:\")\n",
    "display(results_df)\n",
    "\n",
    "accuracy = results_df[\"correct\"].mean()\n",
    "print(f\"\\nModel accuracy on test cases: {accuracy:.1%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
