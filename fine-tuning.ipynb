{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kev\\Documents\\GitHub\\bias-detector-en-de\\thesis-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# torch and dataset utils\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# transformers library\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# evaluation and data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e222663",
   "metadata": {},
   "source": [
    "# Seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed839ed",
   "metadata": {},
   "source": [
    "calls Python’s random.seed, NumPy’s np.random.seed and PyTorch’s torch.manual_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                  # Python built-in RNG\n",
    "    np.random.seed(seed)               # NumPy RNG\n",
    "    torch.manual_seed(seed)            # PyTorch RNG (CPU)\n",
    "    torch.cuda.manual_seed(seed)       # PyTorch RNG (current GPU)\n",
    "    torch.cuda.manual_seed_all(seed)   # All GPUs (if using multi-GPU)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6a176",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a751c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/dataset_f.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f54aa9",
   "metadata": {},
   "source": [
    "# Load pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print device info\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU: None (using CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc0e30",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba47b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_path = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e99331",
   "metadata": {},
   "source": [
    "## Load Model and Send to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model with classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"neutral\", 1: \"biased\"},\n",
    "    label2id={\"neutral\": 0, \"biased\": 1}\n",
    ")\n",
    "\n",
    "# move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb879bcf",
   "metadata": {},
   "source": [
    "# Set trainable parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4f305",
   "metadata": {},
   "source": [
    "- \"transfer learning\". we leave the base model parameters frozen, only train a classification head that we add on top\n",
    "- might result in rigid model\n",
    "- unfreeze final four layers, keeping computational cost down but keep flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 14767874\n"
     ]
    }
   ],
   "source": [
    "# freeze encoder layers 0 to 7\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"bert.encoder.layer.\"):\n",
    "        layer_num = int(name.split(\".\")[3])\n",
    "        if layer_num <= 7:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# count and print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00540c44",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd93d9",
   "metadata": {},
   "source": [
    "- PyTorch models need input data in a specific format\n",
    "- BiasDataset class turns each row from df into tokenized input tensors for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset for bias detection\n",
    "class BiasDataset(Dataset):\n",
    "    # init with dataframe and tokenizer\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # return number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # return one encoded sample\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.data.iloc[idx][\"english\"]\n",
    "        german = self.data.iloc[idx][\"german\"]\n",
    "        label = int(self.data.iloc[idx][\"label\"])\n",
    "\n",
    "        # tokenize EN-DE sentence pair\n",
    "        encoded = self.tokenizer(\n",
    "            text=english,\n",
    "            text_pair=german,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # encoded outputs have extra batch dimension, remove it with squeeze(0) to get plain tensors\n",
    "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
    "        # add label tensor to the dict under key 'labels'\n",
    "        item[\"labels\"] = torch.tensor(label)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bec775",
   "metadata": {},
   "source": [
    "- tokenizer gives tensors with a first size of 1 (a batch)\n",
    "- squeeze(0) removes that first size, making single samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc28df7",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e171490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train (80%) and temp (20%), keeping label distribution same with stratify\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# split temp into validation (10%) and test (10%) sets, stratified by label\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06d805",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validation and test datasets\n",
    "train_dataset = BiasDataset(train_df, tokenizer)  \n",
    "val_dataset = BiasDataset(val_df, tokenizer)    \n",
    "test_dataset = BiasDataset(test_df, tokenizer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28148ce4",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Calculation for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c893c9f",
   "metadata": {},
   "source": [
    "The `compute_metrics` function calculates evaluation metrics for classification results.\n",
    "\n",
    "### Inputs\n",
    "- `eval_pred`: A tuple containing two elements:\n",
    "  - `logits`: The raw model outputs (before softmax).\n",
    "  - `labels`: The true class labels.\n",
    "\n",
    "### Process\n",
    "1. Get predicted classes by taking the index with the highest logit value.\n",
    "2. Calculate precision, recall, F1 score, and support for each class separately.\n",
    "3. Create a detailed classification report as a formatted string.\n",
    "4. Compute the confusion matrix showing correct and incorrect predictions per class.\n",
    "5. Prepare a dictionary of metrics including:\n",
    "   - Precision, recall, F1, and support for each class.\n",
    "   - Macro averages (mean) for precision, recall, and F1.\n",
    "   - Overall accuracy.\n",
    "6. Print the confusion matrix and classification report.\n",
    "\n",
    "### Output\n",
    "- Returns a dictionary with all the above metrics, suitable for logging or further analysis.\n",
    "\n",
    "### Notes\n",
    "- `average=None` means metrics are computed separately for each class.\n",
    "- `zero_division=0` avoids errors if some classes have no predicted samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        # per-class\n",
    "        **{f\"precision_class_{i}\": precision[i] for i in range(len(precision))},\n",
    "        **{f\"recall_class_{i}\":    recall[i]    for i in range(len(recall))},\n",
    "        **{f\"f1_class_{i}\":        f1[i]        for i in range(len(f1))},\n",
    "        **{f\"support_class_{i}\":   support[i]   for i in range(len(support))},\n",
    "        # overall\n",
    "        \"precision_macro\": np.mean(precision),\n",
    "        \"recall_macro\":    np.mean(recall),\n",
    "        \"f1_macro\":        np.mean(f1),\n",
    "        \"accuracy\":        (predictions == labels).mean(),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d441df9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be9511",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-5\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "\n",
    "output_dir=\"./model_output_dataset_f\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    seed = seed,\n",
    "    output_dir=output_dir,       \n",
    "    num_train_epochs=num_epochs,   \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    learning_rate=lr,             \n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",       \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",  \n",
    "    greater_is_better=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c91741",
   "metadata": {},
   "source": [
    "## Run trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d406a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fcdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 03:21, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>0.512544</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.791519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.428950</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.847328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.361369</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.870229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.330381</td>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.787671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.263788</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.908367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.230804</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.864662</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.236226</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.930612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.259122</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_output_dataset_f\\\\tokenizer_config.json',\n",
       " './model_output_dataset_f\\\\special_tokens_map.json',\n",
       " './model_output_dataset_f\\\\vocab.txt',\n",
       " './model_output_dataset_f\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "try:\n",
    "    train_results = trainer.train()\n",
    "except Exception as e:\n",
    "    print(\"Training failed:\", e)\n",
    "    raise\n",
    "\n",
    "print(\"Training complete. Saving model...\")\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e28c",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7253a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.931\n",
      "Test F1: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "\n",
    "# get metrics only from validation\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation F1:\", round(val_results[\"eval_f1\"], 3))\n",
    "\n",
    "# get predictions and labels for validation\n",
    "val_preds_output = trainer.predict(val_dataset)\n",
    "val_logits = val_preds_output.predictions\n",
    "val_labels = val_preds_output.label_ids\n",
    "val_preds = np.argmax(val_logits, axis=1)\n",
    "\n",
    "# print confusion matrix and classification report for validation\n",
    "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(val_labels, val_preds))\n",
    "print(\"\\nValidation Classification Report:\\n\", classification_report(val_labels, val_preds, zero_division=0, digits=4))\n",
    "\n",
    "\n",
    "# same for test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test F1:\", round(test_results[\"eval_f1\"], 3))\n",
    "\n",
    "test_preds_output = trainer.predict(test_dataset)\n",
    "test_logits = test_preds_output.predictions\n",
    "test_labels = test_preds_output.label_ids\n",
    "test_preds = np.argmax(test_logits, axis=1)\n",
    "\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_preds))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(test_labels, test_preds, zero_division=0, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d97164",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911a70a",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f1b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = output_dir\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a459cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten text helper function\n",
    "def shorten(text, max_len=40):\n",
    "    if len(text) <= max_len:\n",
    "        return text\n",
    "    return text[:max_len-3] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8b85d",
   "metadata": {},
   "source": [
    "## My Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # neutral\n",
    "    (\"Today the weather is beautiful\", \"Heute ist das Wetter schön\", 0), \n",
    "    (\"I like this guy.\", \"Ich mag diesen Kerl.\", 0),\n",
    "    (\"How are you?\", \"Wie geht es dir?\", 0),\n",
    "    (\"The girls went hiking.\", \"Die Mädchen gingen wandern.\", 0),\n",
    "\n",
    "    # neutral with gendered subjects\n",
    "    (\"My mother is an engineer.\", \"Meine Mutter ist Ingenieurin.\", 0),\n",
    "    (\"Is this boy a good artist?\", \"Ist dieser Junge ein guter Künstler?\", 0),\n",
    "    (\"I am living with my sister, who is also my best friend\", \"Ich lebe mit meiner Schwester, die auch meine beste Freundin ist\", 0),\n",
    "    (\"My father was an excellent cook.\", \"Mein Vater war ein ausgezeichneter Koch.\", 0),\n",
    "\n",
    "    # biased\n",
    "    (\"Do you like our maths teacher?\", \"Mögen Sie unsere Mathelehrerin?\", 1),\n",
    "    (\"The doctor was late to work today.\", \"Der Arzt kam heute zu spät zur Arbeit.\", 1),\n",
    "    (\"Tomorrow the students are leaving for a class trip.\", \"Morgen gehen die Studenten zu einer Klassenfahrt.\", 1),\n",
    "    (\"Engineers are great.\", \"Ingenieure sind großartig\", 1),\n",
    "    (\"Engineers earn a lot of money.\", \"Ingenieure verdienen viel Geld.\", 1),\n",
    "\n",
    "    # test GFL\n",
    "    (\"Engineers are kind and hard-working\", \"Ingenieur:innen sind lieb und fleißig\", 0),\n",
    "    (\"Engineers are kind and hard-working\", \"Ingenieure und Ingenieurinnen sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrende sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrer:innen sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrerinnen und Lehrer sind lieb und fleißig\", 0),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrer sind lieb und fleißig\", 1),\n",
    "    (\"Teachers are kind and hard-working\", \"Lehrerinnen sind lieb und fleißig\", 1),\n",
    "\n",
    "    # job posting\n",
    "    (\"Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another.\", \"Googles Software-Ingenieure entwickeln die Technologien der nächsten Generation, die ändern, wie Milliarden von Benutzern verbinden, erkunden, und interagieren mit Informationen und einander.\", 1),\n",
    "    (\"The ideal candidate should have a solid technical foundation with a focus on Custom agent development and Copilot integrations, strategic thinking, excellent communication skills, and the ability to collaborate within a global team.\", \"Der ideale Kandidat sollte über solide technische Grundlagen mit Schwerpunkt auf der Entwicklung kundenspezifischer Agenten und Copilot-Integrationen, strategisches Denken, ausgezeichnete Kommunikationsfähigkeiten und die Fähigkeit zur Zusammenarbeit in einem globalen Team verfügen.\", 1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of test cases into a dataframe\n",
    "test_df = pd.DataFrame(test_cases, columns=[\"english\", \"german\", \"label\"])\n",
    "\n",
    "# create BiasDataset instance from dataframe\n",
    "test_dataset = BiasDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86608a33",
   "metadata": {},
   "source": [
    "## Run Inference on each Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14badba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    item = test_dataset[i]\n",
    "    \n",
    "    # prepare inputs for model, add batch dimension and move to device\n",
    "    inputs = {key: val.unsqueeze(0).to(device) for key, val in item.items() if key != \"labels\"}\n",
    "    \n",
    "    # run model in evaluation mode without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_label = torch.argmax(logits, dim=1).item()\n",
    "        prob = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # collect results\n",
    "    results.append({\n",
    "        \"english\": test_df.iloc[i][\"english\"],\n",
    "        \"german\": test_df.iloc[i][\"german\"],\n",
    "        \"true_label\": test_df.iloc[i][\"label\"],\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"neutral_prob\": prob[0],\n",
    "        \"biased_prob\": prob[1],\n",
    "        \"correct\": test_df.iloc[i][\"label\"] == pred_label\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0369a8",
   "metadata": {},
   "source": [
    "## Display Results as Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nBias detection test results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "accuracy = results_df[\"correct\"].mean()\n",
    "print(f\"\\nModel accuracy on test cases: {accuracy:.1%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
