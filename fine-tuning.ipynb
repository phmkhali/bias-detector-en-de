{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8487e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kev\\Documents\\GitHub\\bias-detector-en-de\\thesis-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e222663",
   "metadata": {},
   "source": [
    "# Seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471a072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6a176",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a751c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9c6e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In my capacity as draftsperson of the Committe...</td>\n",
       "      <td>Als Berichterstatterin des Petitionsausschusse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The recipients are responsible.</td>\n",
       "      <td>Die Rezipierenden sind verantwortlich.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never pay any attention to me.</td>\n",
       "      <td>Nie zollten sie mir eine Aufmerksamkeit.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deputy Mrs Izquierdo Rojo, I have outlined the...</td>\n",
       "      <td>Ich habe das Parlamentsmitglied Izquierdo Rojo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to address Commissioner Verheugen perso...</td>\n",
       "      <td>Ich möchte mich mit einigen Bemerkungen an das...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  In my capacity as draftsperson of the Committe...   \n",
       "1                    The recipients are responsible.   \n",
       "2                They never pay any attention to me.   \n",
       "3  Deputy Mrs Izquierdo Rojo, I have outlined the...   \n",
       "4  I want to address Commissioner Verheugen perso...   \n",
       "\n",
       "                                              german  label  \n",
       "0  Als Berichterstatterin des Petitionsausschusse...      1  \n",
       "1             Die Rezipierenden sind verantwortlich.      0  \n",
       "2           Nie zollten sie mir eine Aufmerksamkeit.      0  \n",
       "3  Ich habe das Parlamentsmitglied Izquierdo Rojo...      1  \n",
       "4  Ich möchte mich mit einigen Bemerkungen an das...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f54aa9",
   "metadata": {},
   "source": [
    "# Load pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f279cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU: None (using CPU)\")\n",
    "# model path\n",
    "model_path = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load model with binary classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"neutral\", 1: \"biased\"},\n",
    "    label2id={\"neutral\": 0, \"biased\": 1}\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb879bcf",
   "metadata": {},
   "source": [
    "# Set trainable parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4f305",
   "metadata": {},
   "source": [
    "- \"transfer learning\". we leave the base model parameters frozen, only train a classification head that we add on top\n",
    "- might result in rigid model\n",
    "- unfreeze final four layers, keeping computational cost down but keep flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bc016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 14767874\n"
     ]
    }
   ],
   "source": [
    "trainable = [\"encoder.layer.10\", \"encoder.layer.11\", \"pooler\", \"classifier\"]\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = any(layer in name for layer in trainable)\n",
    "\n",
    "# log param counts\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00540c44",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd93d9",
   "metadata": {},
   "source": [
    "- PyTorch models need input data in a specific format\n",
    "- BiasDataset class turns each row from df into tokenized input tensors for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasDataset(Dataset):\n",
    "    # store df and tokenizer\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # how many samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # runs every time model needs one item from dataset\n",
    "    # grabs english and german sentence, tokenizes them as a pair, applied padding, trunc and max_length, converts into pytorch tensors, returns a dict\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.data.iloc[idx][\"english\"]\n",
    "        german = self.data.iloc[idx][\"german\"]\n",
    "        label = int(self.data.iloc[idx][\"label\"])\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text=english,\n",
    "            text_pair=german,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "            return_overflowing_tokens=False\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
    "        item[\"labels\"] = torch.tensor(label)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bec775",
   "metadata": {},
   "source": [
    "- tokenizer gives tensors with a first size of 1 (a batch)\n",
    "- squeeze(0) removes that first size, making single samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc28df7",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e171490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=seed\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_df[\"label\"], \n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06d805",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80e26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BiasDataset(train_df, tokenizer)  \n",
    "val_dataset = BiasDataset(val_df, tokenizer)    \n",
    "test_dataset = BiasDataset(test_df, tokenizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28148ce4",
   "metadata": {},
   "source": [
    "# Define evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c893c9f",
   "metadata": {},
   "source": [
    "- **`evaluate` function** runs the model on test data to check performance.\n",
    "\n",
    "- `model.eval()`  \n",
    "  - Sets the model to evaluation mode (no training or dropout).\n",
    "\n",
    "- Initialize empty lists:  \n",
    "  - `all_labels` to save true labels.  \n",
    "  - `all_preds` to save predicted labels.\n",
    "\n",
    "- Loop through batches in `dataloader`:  \n",
    "  - Move inputs and labels to device (CPU/GPU).  \n",
    "  - Get model outputs (logits).  \n",
    "  - Select predicted class with highest score (`argmax`).  \n",
    "  - Add true labels and predictions to lists.\n",
    "\n",
    "- After the loop:  \n",
    "  - Calculate **accuracy**: percentage of correct predictions.  \n",
    "  - Calculate **precision**: correct biased predictions / all biased predictions made.  \n",
    "  - Calculate **recall**: correct biased predictions / all actual biased samples.  \n",
    "  - Calculate **f1-score**: balance between precision and recall.\n",
    "\n",
    "- Return all four metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e6231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d441df9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be9511",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dd9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-5\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    seed = seed,\n",
    "    output_dir=\"./model_output\",       \n",
    "    num_train_epochs=num_epochs,   \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    learning_rate=lr,             \n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",       \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",  \n",
    "    greater_is_better=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c91741",
   "metadata": {},
   "source": [
    "## Run trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d406a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80fcdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='2080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/2080 05:17 < 01:46, 4.90 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>0.474895</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.561905</td>\n",
       "      <td>0.608247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>0.399995</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.693966</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.728507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>0.322704</td>\n",
       "      <td>0.844231</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.812933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.269606</td>\n",
       "      <td>0.878846</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.248411</td>\n",
       "      <td>0.878846</td>\n",
       "      <td>0.851675</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.849642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.235468</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.866337</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.849515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_output\\\\tokenizer_config.json',\n",
       " './model_output\\\\special_tokens_map.json',\n",
       " './model_output\\\\vocab.txt',\n",
       " './model_output\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "try:\n",
    "    train_results = trainer.train()\n",
    "except Exception as e:\n",
    "    print(\"Training failed:\", e)\n",
    "    raise\n",
    "\n",
    "print(\"Training complete. Saving model...\")\n",
    "\n",
    "trainer.save_model(\"./model_output\")\n",
    "tokenizer.save_pretrained(\"./model_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e28c",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b7253a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.857\n",
      "Test F1: 0.825\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation F1:\", round(val_results[\"eval_f1\"], 3))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test F1:\", round(test_results[\"eval_f1\"], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d97164",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099f1b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./model_output\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bias Detection Test Cases:\n",
      "                           English                                 German  True Label  Predicted Label  Neutral Prob  Biased Prob  Correct\n",
      "                 The nurse is kind    Die Krankenschwester ist freundlich           1                1      0.284066     0.715934     True\n",
      "                The nurse is sweet         Die Pflegekraft ist freundlich           0                1      0.142957     0.857043    False\n",
      "              The doctor is strong                     Der Arzt ist stark           1                1      0.139930     0.860070     True\n",
      "    Today the weather is beautiful             Heute ist das Wetter schön           0                0      0.999195     0.000805     True\n",
      "              The woman is a coder      Die Frau ist eine Programmiererin           0                0      0.886548     0.113452     True\n",
      "The teachers are not working today     Die Lehrenden arbeiten heute nicht           0                0      0.705959     0.294041     True\n",
      "The advisors are not working today Die Berater*innen arbeiten heute nicht           0                1      0.029542     0.970458    False\n",
      "            The teachers are smart                 Die Lehrer sind schlau           1                1      0.025560     0.974440     True\n",
      "                         I am fine                        Mir geht es gut           0                0      0.998978     0.001023     True\n",
      "\n",
      "Model Accuracy on Test Sentences: 77.8%\n"
     ]
    }
   ],
   "source": [
    "# Define test cases (English, German) with expected bias labels\n",
    "test_cases = [\n",
    "    (\"The nurse is kind\", \"Die Krankenschwester ist freundlich\", 1),  \n",
    "    (\"The nurse is sweet\", \"Die Pflegekraft ist freundlich\", 0),       \n",
    "    (\"The doctor is strong\", \"Der Arzt ist stark\", 1),               \n",
    "    (\"Today the weather is beautiful\", \"Heute ist das Wetter schön\", 0), \n",
    "    (\"The woman is a coder\", \"Die Frau ist eine Programmiererin\", 0), \n",
    "    (\"The teachers are not working today\", \"Die Lehrenden arbeiten heute nicht\", 0), \n",
    "    (\"The advisors are not working today\", \"Die Berater*innen arbeiten heute nicht\", 0), \n",
    "    (\"The teachers are smart\", \"Die Lehrer sind schlau\", 1), \n",
    "    (\"I am fine\", \"Mir geht es gut\",0),\n",
    "    (\"I am helping the cleaner\", \"Ich helfe der Putzfrau\",1),\n",
    "    (\"I am helping the cleaner\", \"Ich helfe der Reinigungskraft\",0)\n",
    "]\n",
    "\n",
    "# Prepare results table\n",
    "results = []\n",
    "\n",
    "# Run predictions\n",
    "for eng, de, true_label in test_cases:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        eng, de,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_label = torch.argmax(outputs.logits).item()\n",
    "        prob = torch.softmax(outputs.logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    results.append({\n",
    "        \"English\": eng,\n",
    "        \"German\": de,\n",
    "        \"True Label\": true_label,\n",
    "        \"Predicted Label\": pred_label,\n",
    "        \"Neutral Prob\": prob[0],\n",
    "        \"Biased Prob\": prob[1],\n",
    "        \"Correct\": true_label == pred_label\n",
    "    })\n",
    "\n",
    "# Display as formatted table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nBias Detection Test Cases:\")\n",
    "print(results_df.to_string(index=False))  # This prints the whole table cleanly\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = results_df[\"Correct\"].mean()\n",
    "print(f\"\\nModel Accuracy on Test Sentences: {accuracy:.1%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
