
@inproceedings{papineni_bleu_2001,
	location = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10.3115/1073083.1073135},
	shorttitle = {{BLEU}},
	eventtitle = {the 40th Annual Meeting},
	pages = {311},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	urldate = {2025-02-27},
	date = {2001},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\S6EE3KSL\\Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine translation.pdf:application/pdf},
}

@incollection{braccini_does_2024,
	location = {Cham},
	title = {Does {AI} Reflect Human Behaviour? Exploring the Presence of Gender Bias in {AI} Translation Tools},
	volume = {72},
	isbn = {978-3-031-75585-9 978-3-031-75586-6},
	url = {https://link.springer.com/10.1007/978-3-031-75586-6_19},
	shorttitle = {Does {AI} Reflect Human Behaviour?},
	abstract = {Natural language processing tools are becoming more and more important in our daily life, enabling us to perform many tasks in a timely and efﬁcient manner. However, as the utilisation of these tools growth, so does the risk of unexpected consequences due to the presence of bias. This study investigates the presence of gender bias within the most popular neural machine translation and large language model tools. We deﬁned a set of Italian sentences concerning ten speciﬁc jobs, where the gender of the subjects is not explicitly mentioned. Employing those {AI} tools, we translated the sentences from Italian to English, requiring the gender to be explicitly mentioned. Afterwards, we developed a survey to obtain human translations for the same sentences, allowing us to compare the differences between the responses generated by the tools and those from individuals. Results show a high presence of gender bias especially for the jobs associated with a male gender and demonstrate a consistency between the outcome obtained by the tools and the results of the survey. These ﬁndings serve as a starting point for exploring the origins of gender bias within natural language processing tools and how they reﬂect gender distributions in our society and human behaviour regarding job occupations.},
	pages = {355--373},
	booktitle = {Digital (Eco) Systems and Societal Challenges},
	publisher = {Springer Nature Switzerland},
	author = {Smacchia, Marco and Za, Stefano and Arenas, Alvaro},
	editor = {Braccini, Alessio Maria and Ricciardi, Francesca and Virili, Francesco},
	urldate = {2025-02-27},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-75586-6_19},
	note = {Series Title: Lecture Notes in Information Systems and Organisation},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\DQXYL28J\\Smacchia et al. - 2024 - Does AI Reflect Human Behaviour Exploring the Presence of Gender Bias in AI Translation Tools.pdf:application/pdf},
}

@inproceedings{unior_nlp_research_group_university_of_naples__gender_2023,
	title = {Gender Bias in Machine Translation: a statistical evaluation of Google Translate and {DeepL} for English, Italian and German},
	url = {https://acl-bg.org/proceedings/2023/HiT-IT%202023/pdf/2023.hitit2023-1.1.pdf},
	doi = {10.26615/issn.2683-0078.2023_001},
	shorttitle = {Gender Bias in Machine Translation},
	abstract = {Despite the significant advancements made in the field of Machine Translation ({MT}) technology, there are still some challenges that need to be addressed. One such challenge is represented by the issue of gender bias in machine translation systems. The main objective of this study is to examine and investigate the presence of gender bias in {MT} systems and identify any potential issues related to the use of sexist language. The research evaluates the performance of Google Translate and {DeepL} in terms of natural gender translation, particularly the frequency of male and female forms used in translating sentences that refer to professions without any other gender-specific words. The evaluation is carried out using the {MT}-{GenEval} corpus [2] contextual subset, for English-Italian and English-German language pairs. The paper presents the statistical findings obtained from the evaluation.},
	eventtitle = {International Conference on Human-informed Translation and Interpreting Technology 2023},
	pages = {1--11},
	booktitle = {Proceedings of the International Conference on Human-informed Translation and Interpreting Technology 2023},
	publisher = {{INCOMA} Ltd., Shoumen, Bulgaria},
	author = {{UNIOR NLP Research Group, University of Naples "} and {L’Orientale"} and {, Naples, Italy} and Rescigno, Argentina Anna and Monti, Johanna and {UNIOR NLP Research Group, University of Naples "L’Orientale", Naples, Italy}},
	urldate = {2025-02-27},
	date = {2023},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\KRUC85NJ\\UNIOR NLP Research Group, University of Naples  et al. - 2023 - Gender Bias in Machine Translation a statistical evaluation of Google Translate and DeepL for Engli.pdf:application/pdf},
}

@misc{gete_does_2024,
	title = {Does Context Help Mitigate Gender Bias in Neural Machine Translation?},
	url = {http://arxiv.org/abs/2406.12364},
	doi = {10.48550/arXiv.2406.12364},
	abstract = {Neural Machine Translation models tend to perpetuate gender bias present in their training data distribution. Context-aware models have been previously suggested as a means to mitigate this type of bias. In this work, we examine this claim by analysing in detail the translation of stereotypical professions in English to German, and translation with non-informative context in Basque to Spanish. Our results show that, although context-aware models can significantly enhance translation accuracy for feminine terms, they can still maintain or even amplify gender bias. These results highlight the need for more fine-grained approaches to bias mitigation in Neural Machine Translation.},
	number = {{arXiv}:2406.12364},
	publisher = {{arXiv}},
	author = {Gete, Harritxu and Etchegoyhen, Thierry},
	urldate = {2025-02-27},
	date = {2024},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.12364 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\ZFTRAPLI\\Gete und Etchegoyhen - 2024 - Does Context Help Mitigate Gender Bias in Neural Machine Translation.pdf:application/pdf},
}

@misc{prates_assessing_2019,
	title = {Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate},
	url = {http://arxiv.org/abs/1809.02208},
	doi = {10.48550/arXiv.1809.02208},
	abstract = {This research paper, "Assessing Gender Bias in Machine Translation – A Case Study with Google Translate", investigates the presence of gender bias within automated translation tools.

The authors explore this by inputting gender-neutral sentences, specifically job titles and adjectives, in various languages into Google Translate and observing the gendered pronouns in the English output. Their findings reveal a strong tendency for Google Translate to default to male pronouns, particularly for professions stereotypically associated with men, and they demonstrate that this bias does not simply reflect real-world gender distributions in the workplace. Ultimately, the study highlights how machine learning systems can inadvertently perpetuate societal biases and calls for the development of debiasing techniques to ensure greater fairness in such technologies.},
	number = {{arXiv}:1809.02208},
	publisher = {{arXiv}},
	author = {Prates, Marcelo O. R. and Avelar, Pedro H. C. and Lamb, Luis},
	urldate = {2025-04-03},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.02208 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\5CDI4C3V\\Prates et al. - 2019 - Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate.pdf:application/pdf},
}

@misc{stanovsky_evaluating_2019,
	title = {Evaluating Gender Bias in Machine Translation},
	url = {http://arxiv.org/abs/1906.00591},
	doi = {10.48550/arXiv.1906.00591},
	abstract = {We present the ﬁrst challenge set and evaluation protocol for the analysis of gender bias in machine translation ({MT}). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inﬂection for the word “doctor”). Our analyses show that four popular industrial {MT} systems and two recent state-of-the-art academic {MT} models are signiﬁcantly prone to genderbiased translation errors for all tested target languages. Our data and code are publicly available at shorturl.at/{dimuD}.},
	number = {{arXiv}:1906.00591},
	publisher = {{arXiv}},
	author = {Stanovsky, Gabriel and Smith, Noah A. and Zettlemoyer, Luke},
	urldate = {2025-04-03},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.00591 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\GNSTFCIL\\Stanovsky et al. - 2019 - Evaluating Gender Bias in Machine Translation.pdf:application/pdf},
}

@article{bolukbasi_man_2016,
	title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
	url = {https://doi.org/10.48550/arXiv.1607.06520},
	abstract = {This research paper addresses the critical issue of gender bias present within word embeddings, a widely used technique in machine learning for representing text. The authors demonstrate that even embeddings trained on seemingly neutral data, like Google News articles, exhibit disturbing gender stereotypes, such as associating "computer programmer" more closely with "man" and "homemaker" with "woman". Recognizing the potential for these biases to be amplified in downstream applications, the paper's main purpose is to develop methodologies to reduce or remove these gender stereotypes from word embeddings. The authors propose and evaluate algorithms that can "debias" the embeddings, aiming to create more equitable representations of language while preserving their usefulness for tasks like analogy solving and semantic understanding.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	date = {2016},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\35RIHQCX\\Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings.pdf:application/pdf},
}

@inproceedings{lardelli_building_2024,
	location = {Bangkok, Thailand and virtual meeting},
	title = {Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into German},
	url = {https://aclanthology.org/2024.findings-acl.448},
	doi = {10.18653/v1/2024.findings-acl.448},
	shorttitle = {Building Bridges},
	abstract = {The translation of gender-neutral personreferring terms (e.g., the students) is often nontrivial. Translating from English into German poses an interesting case—in German, personreferring nouns are usually gender-specific, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used. This solution, however, reduces the visibility of other genders, such as women and non-binary people. To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems). However, gender-fair German is currently barely supported in machine translation ({MT}), requiring post-editing or manual translations. We address this research gap by studying gender-fair language in English-to-German {MT}. Concretely, we enrich a community-created gender-fair language dictionary and sample multi-sentence test instances from encyclopedic text and parliamentary speeches. Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural {MT} models for translating words in isolation and natural contexts across two domains. Our findings show that most systems produce mainly masculine forms and rarely genderneutral variants, highlighting the need for future research. We release code and data at https://github.com/g8a9/building-b ridges-gender-fair-german-mt.},
	eventtitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	pages = {7542--7550},
	booktitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Lardelli, Manuel and Attanasio, Giuseppe and Lauscher, Anne},
	urldate = {2025-04-06},
	date = {2024},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\BT3GSQC7\\Lardelli et al. - 2024 - Building Bridges A Dataset for Evaluating Gender-Fair Machine Translation into German.pdf:application/pdf},
}

@inproceedings{savoldi_what_2024,
	location = {Miami, Florida, {USA}},
	title = {What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study},
	url = {https://aclanthology.org/2024.emnlp-main.1002},
	doi = {10.18653/v1/2024.emnlp-main.1002},
	shorttitle = {What the Harm?},
	abstract = {Gender bias in machine translation ({MT}) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final {MT} users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in {MT} brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from ∼90 participants, who post-edited {MT} outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.},
	eventtitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	pages = {18048--18076},
	booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Savoldi, Beatrice and Papi, Sara and Negri, Matteo and Guerberof-Arenas, Ana and Bentivogli, Luisa},
	urldate = {2025-04-06},
	date = {2024},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\VIZLY5NZ\\Savoldi et al. - 2024 - What the Harm Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-ce.pdf:application/pdf},
}

@article{shrestha_exploring_2022,
	title = {Exploring gender biases in {ML} and {AI} academic research through systematic literature review},
	volume = {5},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2022.976838/full},
	doi = {10.3389/frai.2022.976838},
	abstract = {Automated systems that implement Machine learning ({ML}) and Artificial Intelligence ({AI}) algorithms present promising solutions to a variety of technological and non-technological issues. Although, industry leaders are rapidly adopting these systems for anything from marketing to national defense operations, these systems are not without flaws. Recently, many of these systems are found to inherit and propagate gender and racial biases that disadvantages the minority population. In this paper, we analyze academic publications in the area of gender biases in {ML} and {AI} algorithms thus outlining different themes, mitigation and detection methods explored through research in this topic. Through a detailed analysis of
              N
              = 120 papers, we map the current research landscape on gender specific biases present in {ML} and {AI} assisted automated systems. We further point out the aspects of {ML}/{AI} gender biases research that are less explored and require more attention. Mainly we focus on the lack of user studies and inclusivity in this field of study. We also shed some light into the gender bias issue as experienced by the algorithm designers. In conclusion, in this paper we provide a holistic view of the breadth of studies conducted in the field of exploring, detecting and mitigating gender biases in {ML} and {AI} systems and, a future direction for the studies to take in order to provide a fair and accessible {ML} and {AI} systems to all users.},
	pages = {976838},
	journaltitle = {Frontiers in Artificial Intelligence},
	shortjournal = {Front. Artif. Intell.},
	author = {Shrestha, Sunny and Das, Sanchari},
	urldate = {2025-04-06},
	date = {2022},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\WRGK82UA\\Shrestha and Das - 2022 - Exploring gender biases in ML and AI academic research through systematic literature review.pdf:application/pdf},
}

@article{godsil_effects_2016,
	title = {The Effects of Gender Roles, Implicit Bias, and Stereotype Threat on the Lives of Women and Girls},
	volume = {2},
	url = {https://equity.ucla.edu/wp-content/uploads/2016/11/Science-of-Equality-Volume-2.pdf},
	issue = {Perception Institute},
	journaltitle = {{THE} {SCIENCE} {OF} {EQUALITY}},
	author = {Godsil, Rachel D. and Tropp, Linda R. and Goff, Phillip Atiba and Powell, John A. and {MacFarlane}, Jessica},
	date = {2016},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\YDMUSE26\\Johnson et al. - PERCEPTION INSTITUTE.pdf:application/pdf},
}

@article{tiedemann_opus-mt_2020,
	title = {{OPUS}-{MT} – Building open translation services for the World},
	volume = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
	url = {https://aclanthology.org/2020.eamt-1.61/},
	pages = {479--480},
	journaltitle = {European Association for Machine Translation},
	author = {Tiedemann, Jorg and Thottingal, Santhosh},
	date = {2020},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\63AA5VDK\\Tiedemann and Thottingal - OPUS-MT – Building open translation services for the World.pdf:application/pdf},
}

@misc{savoldi_mgente_2025,
	title = {{mGeNTE}: A Multilingual Resource for Gender-Neutral Language and Translation},
	url = {http://arxiv.org/abs/2501.09409},
	doi = {10.48550/arXiv.2501.09409},
	shorttitle = {{mGeNTE}},
	abstract = {Gender-neutral language reflects societal and linguistic shifts towards greater inclusivity by avoiding the implication that one gender is the norm over others. This is particularly relevant for grammatical gender languages, which heavily encode the gender of terms for human referents and over-relies on masculine forms, even when gender is unspecified or irrelevant. Language technologies are known to mirror these inequalities, being affected by a male bias and perpetuating stereotypical associations when translating into languages with extensive gendered morphology. In such cases, genderneutral language can help avoid undue binary assumptions. However, despite its importance for creating fairer multi- and cross-lingual technologies, inclusive language research remains scarce and insufficiently supported in current resources. To address this gap, we present the multilingual {mGeNTe} dataset. Derived from the bilingual {GeNTE} (Piergentili et al., 2023b), {mGeNTE} extends the original corpus to include the English-Italian/German/Spanish language pairs. Since each language pair is Englishaligned with gendered and neutral sentences in the target languages, {mGeNTE} enables research in both automatic Gender-Neutral Translation ({GNT}) and language modelling for three grammatical gender languages.},
	number = {{arXiv}:2501.09409},
	publisher = {{arXiv}},
	author = {Savoldi, Beatrice and Cupin, Eleonora and Thind, Manjinder and Lauscher, Anne and Piergentili, Andrea and Negri, Matteo and Bentivogli, Luisa},
	urldate = {2025-04-08},
	date = {2025},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2501.09409 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\GHVZT88E\\Savoldi et al. - 2025 - mGeNTE A Multilingual Resource for Gender-Neutral Language and Translation.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2025-04-09},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\BIXCSRQ4\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@misc{kappl_are_2025,
	title = {Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine Translation},
	url = {http://arxiv.org/abs/2502.19104},
	doi = {10.48550/arXiv.2502.19104},
	shorttitle = {Are All Spanish Doctors Male?},
	abstract = {We present {WinoMTDE}, a new gender bias evaluation test set designed to assess occupational stereotyping and underrepresentation in German machine translation ({MT}) systems. Building on the automatic evaluation method introduced by Stanovsky et al. (2019), we extend the approach to German, a language with grammatical gender. The {WinoMTDE} dataset comprises 288 German sentences that are balanced in regard to gender, as well as stereotype, which was annotated using German labor statistics. We conduct a largescale evaluation of five widely used {MT} systems and a large language model. Our results reveal persistent bias in most models, with the {LLM} outperforming traditional systems. The dataset and evaluation code are publicly available at https://github.com/ michellekappl/mt\_gender\_german.},
	number = {{arXiv}:2502.19104},
	publisher = {{arXiv}},
	author = {Kappl, Michelle},
	urldate = {2025-04-10},
	date = {2025},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2502.19104 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\X53BVETT\\Kappl - 2025 - Are All Spanish Doctors Male Evaluating Gender Bias in German Machine Translation.pdf:application/pdf},
}

@online{stella_dataset_2021,
	title = {A Dataset for Studying Gender Bias in Translation},
	url = {https://research.google/blog/a-dataset-for-studying-gender-bias-in-translation/},
	abstract = {Posted by Romina Stella, Product Manager, Google Translate Advances on neural machine translation ({NMT}) have enabled more natural and fluid transla...},
	author = {Stella, Romina and Austermann, Anja and Johnson, Melvin and Linch, Michelle and Niu, Mengmeng and Pushkarna, Mahima and Shah, Apu and Webster, Kellie},
	urldate = {2025-04-10},
	date = {2021},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Kev\\Zotero\\storage\\4E6IHKL2\\a-dataset-for-studying-gender-bias-in-translation.html:text/html},
}

@misc{cho_measuring_2019,
	title = {On Measuring Gender Bias in Translation of Gender-neutral Pronouns},
	url = {http://arxiv.org/abs/1905.11684},
	doi = {10.48550/arXiv.1905.11684},
	abstract = {Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to deﬁne. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with genderneutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index ({TGBI}). The corpus and the code for evaluation is available on-line1.},
	number = {{arXiv}:1905.11684},
	publisher = {{arXiv}},
	author = {Cho, Won Ik and Kim, Ji Won and Kim, Seok Min and Kim, Nam Soo},
	urldate = {2025-04-21},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.11684 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\EPSS6DEE\\Cho et al. - 2019 - On Measuring Gender Bias in Translation of Gender-neutral Pronouns.pdf:application/pdf},
}

@article{currey_mt-geneval_2022,
	title = {{MT}-{GenEval}: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation},
	url = {https://aclanthology.org/2022.emnlp-main.288/},
	doi = {10.18653/v1/2022.emnlp-main.288},
	pages = {4287--4299},
	issue = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
	journaltitle = {Association for Computational Linguistics},
	author = {Currey, Anna and Na, Maria and Lauly, Stanislas and Niu, Xing and Hsu, Benjamin and Dinu, Georgiana},
	date = {2022},
	langid = {english},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\DYWPD9BC\\Currey et al. - MT-GenEval A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Transl.pdf:application/pdf},
}

@online{noauthor_vorlage_nodate,
	title = {Vorlage Expose},
	url = {https://www.overleaf.com/project/67ed0a04d9a03067f037255d},
	abstract = {An online {LaTeX} editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of {LaTeX} templates, and more.},
	urldate = {2025-04-25},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Kev\\Zotero\\storage\\24R2UWUV\\67ed0a04d9a03067f037255d.html:text/html},
}

@misc{pecher_comparing_2024,
	title = {Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance},
	url = {http://arxiv.org/abs/2402.12819},
	doi = {10.48550/arXiv.2402.12819},
	shorttitle = {Comparing Specialised Small and General Large Language Models on Text Classification},
	abstract = {When solving {NLP} tasks with limited labelled data, researchers can either use a general large language model without further update, or use a small number of labelled examples to tune a specialised smaller model. In this work, we address the research gap of how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of finetuning, instruction-tuning, prompting and incontext learning on 7 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average 10 − 1000) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with this number being significantly lower on multi-class datasets (up to 100) than on binary datasets (up to 5000). When performance variance is taken into consideration, the number of required labels increases on average by 100 − 200\% and even up to 1500\% in specific cases.},
	number = {{arXiv}:2402.12819},
	publisher = {{arXiv}},
	author = {Pecher, Branislav and Srba, Ivan and Bielikova, Maria},
	urldate = {2025-04-27},
	date = {2024-04-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.12819 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\Kev\\Zotero\\storage\\JUV3TPBQ\\Pecher et al. - 2024 - Comparing Specialised Small and General Large Language Models on Text Classification 100 Labelled S.pdf:application/pdf},
}
