@incollection{baldiEnglishIndoEuropeanLanguage2008,
  title = {English as an {{Indo}}-{{European Language}}},
  booktitle = {A {{Companion}} to the {{History}} of the {{English Language}}},
  author = {Baldi, Philip},
  editor = {Momma, Haruko and Matto, Michael},
  year = {2008},
  month = jul,
  edition = {1},
  pages = {127--141},
  publisher = {Wiley},
  doi = {10.1002/9781444302851.ch12},
  urldate = {2025-06-06},
  isbn = {978-1-4051-2992-3 978-1-4443-0285-1},
  langid = {english},
  file = {/Users/khali/Zotero/storage/FG6GRZHI/Baldi - 2008 - English as an Indo‚ÄêEuropean Language.pdf}
}

@misc{barclayInvestigatingMarkersDrivers2024a,
  title = {Investigating {{Markers}} and {{Drivers}} of {{Gender Bias}} in {{Machine Translations}}},
  author = {Barclay, Peter J. and Sami, Ashkan},
  year = {2024},
  month = apr,
  number = {arXiv:2403.11896},
  eprint = {2403.11896},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.11896},
  urldate = {2025-05-21},
  abstract = {Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some LLMs use heuristics or postprocessing to mask such bias, making investigation difficult. Here, we examine bias in LLMs via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with `she', and is translated first into a `genderless' intermediate language then back into English; we then examine pronoun-choice in the backtranslated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated translations, avoiding the overinterpretation of individual pronouns, apparent in earlier work; (3) by investigating sentence features that drive bias; (4) and by comparing results from three time-lapsed datasets to establish the reproducibility of the approach. We found that some languages display similar patterns of pronoun use, falling into three loose groups, but that patterns vary between groups; this underlines the need to work with multiple languages. We also identify the main verb appearing in a sentence as a likely significant driver of implied gender in the translations. Moreover, we see a good level of replicability in the results, and establish that our variation metric proves robust despite an obvious change in the behaviour of the DeepL translation API during the course of the study. These results show that the back-translation method can provide further insights into bias in language models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Software Engineering},
  file = {/Users/khali/Zotero/storage/P2DCYJJH/Barclay and Sami - 2024 - Investigating Markers and Drivers of Gender Bias in Machine Translations.pdf}
}

@article{bolukbasiManComputerProgrammer2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  journal = {NIPS'16: Proceedings of the 30th International Conference on Neural Information Processing Systems},
  abstract = {This research paper addresses the critical issue of gender bias present within word embeddings, a widely used technique in machine learning for representing text. The authors demonstrate that even embeddings trained on seemingly neutral data, like Google News articles, exhibit disturbing gender stereotypes, such as associating "computer programmer" more closely with "man" and "homemaker" with "woman". Recognizing the potential for these biases to be amplified in downstream applications, the paper's main purpose is to develop methodologies to reduce or remove these gender stereotypes from word embeddings. The authors propose and evaluate algorithms that can "debias" the embeddings, aiming to create more equitable representations of language while preserving their usefulness for tasks like analogy solving and semantic understanding.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/35RIHQCX/Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings.pdf}
}

@misc{BreakingBarriersBuilding,
  title = {Breaking {{Barriers}}, {{Building Success}}: {{The Power}} of {{Gender Inclusivity}} in {{Business}}},
  shorttitle = {Breaking {{Barriers}}, {{Building Success}}},
  journal = {UNDP},
  urldate = {2025-05-28},
  howpublished = {https://www.undp.org/asia-pacific/blog/breaking-barriers-building-success-power-gender-inclusivity-business},
  langid = {english}
}

@misc{choMeasuringGenderBias2019,
  title = {On {{Measuring Gender Bias}} in {{Translation}} of {{Gender-neutral Pronouns}}},
  author = {Cho, Won Ik and Kim, Ji Won and Kim, Seok Min and Kim, Nam Soo},
  year = {2019},
  number = {arXiv:1905.11684},
  eprint = {1905.11684},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.11684},
  urldate = {2025-04-21},
  abstract = {Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to define. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with genderneutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index (TGBI). The corpus and the code for evaluation is available on-line1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/EPSS6DEE/Cho et al. - 2019 - On Measuring Gender Bias in Translation of Gender-neutral Pronouns.pdf}
}

@article{curreyMTGenEvalCounterfactualContextual2022,
  title = {{{MT-GenEval}}: {{A Counterfactual}} and {{Contextual Dataset}} for {{Evaluating Gender Accuracy}} in {{Machine Translation}}},
  author = {Currey, Anna and Na, Maria and Lauly, Stanislas and Niu, Xing and Hsu, Benjamin and Dinu, Georgiana},
  year = {2022},
  journal = {Association for Computational Linguistics},
  number = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages = {4287--4299},
  doi = {10.18653/v1/2022.emnlp-main.288},
  langid = {english},
  file = {/Users/khali/Zotero/storage/DYWPD9BC/Currey et al. - MT-GenEval A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Transl.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2025-04-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/BIXCSRQ4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@misc{DonnaBurkeGlassy,
  title = {Donna {{Burke}} - {{Glassy Sky}} ({{Chords}})},
  urldate = {2025-06-04},
  abstract = {CHORDS (ver 2) by Donna Burke},
  langid = {english},
  file = {/Users/khali/Zotero/storage/IWSEILWU/glassy-sky-chords-1742215.html}
}

@misc{geteDoesContextHelp2024,
  title = {Does {{Context Help Mitigate Gender Bias}} in {{Neural Machine Translation}}?},
  author = {Gete, Harritxu and Etchegoyhen, Thierry},
  year = {2024},
  number = {arXiv:2406.12364},
  eprint = {2406.12364},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12364},
  urldate = {2025-02-27},
  abstract = {Neural Machine Translation models tend to perpetuate gender bias present in their training data distribution. Context-aware models have been previously suggested as a means to mitigate this type of bias. In this work, we examine this claim by analysing in detail the translation of stereotypical professions in English to German, and translation with non-informative context in Basque to Spanish. Our results show that, although context-aware models can significantly enhance translation accuracy for feminine terms, they can still maintain or even amplify gender bias. These results highlight the need for more fine-grained approaches to bias mitigation in Neural Machine Translation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/ZFTRAPLI/Gete und Etchegoyhen - 2024 - Does Context Help Mitigate Gender Bias in Neural Machine Translation.pdf}
}

@misc{Goal5Department,
  title = {Goal 5 {\textbar} {{Department}} of {{Economic}} and {{Social Affairs}}},
  urldate = {2025-05-28},
  howpublished = {https://sdgs.un.org/goals/goal5},
  file = {/Users/khali/Zotero/storage/NK6ZZK3U/undefined}
}

@article{godsilEffectsGenderRoles2016,
  title = {The {{Effects}} of {{Gender Roles}}, {{Implicit Bias}}, and {{Stereotype Threat}} on the {{Lives}} of {{Women}} and {{Girls}}},
  author = {Godsil, Rachel D. and Tropp, Linda R. and Goff, Phillip Atiba and Powell, John A. and MacFarlane, Jessica},
  year = {2016},
  journal = {THE SCIENCE OF EQUALITY},
  volume = {2},
  number = {Perception Institute},
  langid = {english},
  file = {/Users/khali/Zotero/storage/YDMUSE26/Johnson et al. - PERCEPTION INSTITUTE.pdf}
}

@misc{googleReducingGenderBias2018,
  title = {Reducing Gender Bias in {{Google Translate}}},
  author = {{Google}},
  year = {2018},
  month = dec,
  journal = {Google},
  urldate = {2025-06-05},
  abstract = {Google Translate is launching gender-specific translations: masculine and feminine results for a gender-neutral query.},
  howpublished = {https://blog.google/products/translate/reducing-gender-bias-google-translate/},
  langid = {american},
  file = {/Users/khali/Zotero/storage/VAP3FIZB/reducing-gender-bias-google-translate.html}
}

@misc{GoogleTranslateNow,
  title = {Google {{Translate}} Now Serves 200 Million People Daily},
  journal = {CNET},
  urldate = {2025-05-23},
  abstract = {The company breaks down language barriers a billion times a day, it reveals at Google I/O. On the to-do list: real-time conversation translation.},
  howpublished = {https://www.cnet.com/tech/services-and-software/google-translate-now-serves-200-million-people-daily/},
  langid = {english},
  file = {/Users/khali/Zotero/storage/ZDM29EUJ/google-translate-now-serves-200-million-people-daily.html}
}

@misc{kapplAreAllSpanish2025,
  title = {Are {{All Spanish Doctors Male}}? {{Evaluating Gender Bias}} in {{German Machine Translation}}},
  shorttitle = {Are {{All Spanish Doctors Male}}?},
  author = {Kappl, Michelle},
  year = {2025},
  number = {arXiv:2502.19104},
  eprint = {2502.19104},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.19104},
  urldate = {2025-04-10},
  abstract = {We present WinoMTDE, a new gender bias evaluation test set designed to assess occupational stereotyping and underrepresentation in German machine translation (MT) systems. Building on the automatic evaluation method introduced by Stanovsky et al. (2019), we extend the approach to German, a language with grammatical gender. The WinoMTDE dataset comprises 288 German sentences that are balanced in regard to gender, as well as stereotype, which was annotated using German labor statistics. We conduct a largescale evaluation of five widely used MT systems and a large language model. Our results reveal persistent bias in most models, with the LLM outperforming traditional systems. The dataset and evaluation code are publicly available at https://github.com/ michellekappl/mt\_gender\_german.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/X53BVETT/Kappl - 2025 - Are All Spanish Doctors Male Evaluating Gender Bias in German Machine Translation.pdf}
}

@inproceedings{lardelliBuildingBridgesDataset2024,
  title = {Building {{Bridges}}: {{A Dataset}} for {{Evaluating Gender-Fair Machine Translation}} into {{German}}},
  shorttitle = {Building {{Bridges}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics ACL}} 2024},
  author = {Lardelli, Manuel and Attanasio, Giuseppe and Lauscher, Anne},
  year = {2024},
  pages = {7542--7550},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand and virtual meeting},
  doi = {10.18653/v1/2024.findings-acl.448},
  urldate = {2025-04-06},
  abstract = {The translation of gender-neutral personreferring terms (e.g., the students) is often nontrivial. Translating from English into German poses an interesting case---in German, personreferring nouns are usually gender-specific, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used. This solution, however, reduces the visibility of other genders, such as women and non-binary people. To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems). However, gender-fair German is currently barely supported in machine translation (MT), requiring post-editing or manual translations. We address this research gap by studying gender-fair language in English-to-German MT. Concretely, we enrich a community-created gender-fair language dictionary and sample multi-sentence test instances from encyclopedic text and parliamentary speeches. Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural MT models for translating words in isolation and natural contexts across two domains. Our findings show that most systems produce mainly masculine forms and rarely genderneutral variants, highlighting the need for future research. We release code and data at https://github.com/g8a9/building-b ridges-gender-fair-german-mt.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/BT3GSQC7/Lardelli et al. - 2024 - Building Bridges A Dataset for Evaluating Gender-Fair Machine Translation into German.pdf}
}

@article{linMachineTranslationAcademic2009,
  title = {Machine {{Translation}} for {{Academic Purposes}}},
  author = {Lin, Grace Hui-chin and Chien, Paul Shih Chieh},
  year = {2009},
  journal = {Proceedings of the International Conference on TESOL and Translation 2009},
  pages = {pp.133-148},
  abstract = {Due to the globalization trend and knowledge boost in the second millennium, multi-lingual translation has become a noteworthy issue. For the purposes of learning knowledge in academic fields, Machine Translation (MT) should be noticed not only academically but also practically. MT should be informed to the translating learners because it is a valuable approach to apply by professional translators for diverse professional fields. For learning translating skills and finding a way to learn and teach through bi-lingual/multilingual translating functions in software, machine translation is an ideal approach that translation trainers, translation learners, and professional translators should be familiar with. In fact, theories for machine translation and computer assistance had been highly valued by many scholars. (e.g., Hutchines, 2003; Thriveni, 2002) Based on MIT's Open Courseware into Chinese that Lee, Lin and Bonk (2007) have introduced, this paper demonstrates how MT can be efficiently applied as a superior way of teaching and learning. This article predicts the translated courses utilizing MT for residents of global village should emerge and be provided soon in industrialized nations and it exhibits an overview about what the current developmental status of MT is, why the MT should be fully applied for academic purpose, such as translating a textbook or teaching and learning a course, and what types of software can be successfully applied. It implies MT should be promoted in Taiwan because its functions of clearly translating the key-words and leading the basic learners to a certain professional field can be proved in MIT.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/2H7RUWAZ/Lin and Chien - Machine Translation for Academic Purposes.pdf}
}

@inproceedings{lucyGenderRepresentationBias2021,
  title = {Gender and {{Representation Bias}} in {{GPT-3 Generated Stories}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Narrative Understanding}}},
  author = {Lucy, Li and Bamman, David},
  editor = {Akoury, Nader and Brahman, Faeze and Chaturvedi, Snigdha and Clark, Elizabeth and Iyyer, Mohit and Martin, Lara J.},
  year = {2021},
  month = jun,
  pages = {48--55},
  publisher = {Association for Computational Linguistics},
  address = {Virtual},
  doi = {10.18653/v1/2021.nuse-1.5},
  urldate = {2025-05-23},
  abstract = {Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3`s perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.},
  file = {/Users/khali/Zotero/storage/MJTNUQ85/Lucy and Bamman - 2021 - Gender and Representation Bias in GPT-3 Generated Stories.pdf}
}

@inproceedings{papineniBLEUMethodAutomatic2001,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}  - {{ACL}} '02},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2001},
  pages = {311},
  publisher = {Association for Computational Linguistics},
  address = {Philadelphia, Pennsylvania},
  doi = {10.3115/1073083.1073135},
  urldate = {2025-02-27},
  langid = {english},
  file = {/Users/khali/Zotero/storage/S6EE3KSL/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine translation.pdf}
}

@misc{pecherComparingSpecialisedSmall2024,
  title = {Comparing {{Specialised Small}} and {{General Large Language Models}} on {{Text Classification}}: 100 {{Labelled Samples}} to {{Achieve Break-Even Performance}}},
  shorttitle = {Comparing {{Specialised Small}} and {{General Large Language Models}} on {{Text Classification}}},
  author = {Pecher, Branislav and Srba, Ivan and Bielikova, Maria},
  year = {2024},
  month = apr,
  number = {arXiv:2402.12819},
  eprint = {2402.12819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.12819},
  urldate = {2025-04-27},
  abstract = {When solving NLP tasks with limited labelled data, researchers can either use a general large language model without further update, or use a small number of labelled examples to tune a specialised smaller model. In this work, we address the research gap of how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of finetuning, instruction-tuning, prompting and incontext learning on 7 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average 10 - 1000) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with this number being significantly lower on multi-class datasets (up to 100) than on binary datasets (up to 5000). When performance variance is taken into consideration, the number of required labels increases on average by 100 - 200\% and even up to 1500\% in specific cases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/khali/Zotero/storage/JUV3TPBQ/Pecher et al. - 2024 - Comparing Specialised Small and General Large Language Models on Text Classification 100 Labelled S.pdf}
}

@misc{pratesAssessingGenderBias2019,
  title = {Assessing {{Gender Bias}} in {{Machine Translation}} -- {{A Case Study}} with {{Google Translate}}},
  author = {Prates, Marcelo O. R. and Avelar, Pedro H. C. and Lamb, Luis},
  year = {2019},
  number = {arXiv:1809.02208},
  eprint = {1809.02208},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1809.02208},
  urldate = {2025-04-03},
  abstract = {This research paper, "Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate", investigates the presence of gender bias within automated translation tools. The authors explore this by inputting gender-neutral sentences, specifically job titles and adjectives, in various languages into Google Translate and observing the gendered pronouns in the English output. Their findings reveal a strong tendency for Google Translate to default to male pronouns, particularly for professions stereotypically associated with men, and they demonstrate that this bias does not simply reflect real-world gender distributions in the workplace. Ultimately, the study highlights how machine learning systems can inadvertently perpetuate societal biases and calls for the development of debiasing techniques to ensure greater fairness in such technologies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/khali/Zotero/storage/5CDI4C3V/Prates et al. - 2019 - Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate.pdf}
}

@inproceedings{rescignoGenderBiasMachine2023,
  title = {Gender {{Bias}} in {{Machine Translation}}: A Statistical Evaluation of {{Google Translate}} and {{DeepL}} for {{English}}, {{Italian}} and {{German}}},
  shorttitle = {Gender {{Bias}} in {{Machine Translation}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Human-informed Translation}} and {{Interpreting Technology}} 2023},
  author = {Rescigno, Argentina Anna and Monti, Johanna},
  year = {2023},
  pages = {1--11},
  publisher = {INCOMA Ltd., Shoumen, Bulgaria},
  address = {UNIOR NLP Research Group, University of Naples "L'Orientale", Naples, Italy},
  doi = {10.26615/issn.2683-0078.2023_001},
  urldate = {2025-02-27},
  abstract = {Despite the significant advancements made in the field of Machine Translation (MT) technology, there are still some challenges that need to be addressed. One such challenge is represented by the issue of gender bias in machine translation systems. The main objective of this study is to examine and investigate the presence of gender bias in MT systems and identify any potential issues related to the use of sexist language. The research evaluates the performance of Google Translate and DeepL in terms of natural gender translation, particularly the frequency of male and female forms used in translating sentences that refer to professions without any other gender-specific words. The evaluation is carried out using the MT-GenEval corpus [2] contextual subset, for English-Italian and English-German language pairs. The paper presents the statistical findings obtained from the evaluation.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/KRUC85NJ/UNIOR NLP Research Group, University of Naples  et al. - 2023 - Gender Bias in Machine Translation a statistical evaluation of Google Translate and DeepL for Engli.pdf}
}

@article{savoldiDecadeGenderBias2025,
  title = {A Decade of Gender Bias in Machine Translation},
  author = {Savoldi, Beatrice and Bastings, Jasmijn and Bentivogli, Luisa and Vanmassenhove, Eva},
  year = {2025},
  month = may,
  journal = {Patterns},
  pages = {101257},
  issn = {26663899},
  doi = {10.1016/j.patter.2025.101257},
  urldate = {2025-06-06},
  abstract = {Gender bias in machine translation (MT) has been studied for over a decade, a time marked by societal, linguistic, and technological shifts. With the early optimism for a quick solution in mind, we review over 100 studies on the topic and uncover a more complex reality---one that resists a simple technical fix. While we identify key trends and advancements, persistent gaps remain. We argue that there is no simple technical solution to bias. Building on insights from our review, we examine the growing prominence of large language models and discuss the challenges and opportunities they present in the context of gender bias and translation. By doing so, we hope to inspire future work in the field to break with past limitations and to be less focused on a technical fix; more user-centric, multilingual, and multiculturally diverse; more personalized; and better grounded in real-world needs.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/IGP8BJEB/Savoldi et al. - 2025 - A decade of gender bias in machine translation.pdf}
}

@misc{savoldiMGeNTEMultilingualResource2025,
  title = {{{mGeNTE}}: {{A Multilingual Resource}} for {{Gender-Neutral Language}} and {{Translation}}},
  shorttitle = {{{mGeNTE}}},
  author = {Savoldi, Beatrice and Cupin, Eleonora and Thind, Manjinder and Lauscher, Anne and Piergentili, Andrea and Negri, Matteo and Bentivogli, Luisa},
  year = {2025},
  number = {arXiv:2501.09409},
  eprint = {2501.09409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09409},
  urldate = {2025-04-08},
  abstract = {Gender-neutral language reflects societal and linguistic shifts towards greater inclusivity by avoiding the implication that one gender is the norm over others. This is particularly relevant for grammatical gender languages, which heavily encode the gender of terms for human referents and over-relies on masculine forms, even when gender is unspecified or irrelevant. Language technologies are known to mirror these inequalities, being affected by a male bias and perpetuating stereotypical associations when translating into languages with extensive gendered morphology. In such cases, genderneutral language can help avoid undue binary assumptions. However, despite its importance for creating fairer multi- and cross-lingual technologies, inclusive language research remains scarce and insufficiently supported in current resources. To address this gap, we present the multilingual mGeNTe dataset. Derived from the bilingual GeNTE (Piergentili et al., 2023b), mGeNTE extends the original corpus to include the English-Italian/German/Spanish language pairs. Since each language pair is Englishaligned with gendered and neutral sentences in the target languages, mGeNTE enables research in both automatic Gender-Neutral Translation (GNT) and language modelling for three grammatical gender languages.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/GHVZT88E/Savoldi et al. - 2025 - mGeNTE A Multilingual Resource for Gender-Neutral Language and Translation.pdf}
}

@inproceedings{savoldiWhatHarmQuantifying2024,
  title = {What the {{Harm}}? {{Quantifying}} the {{Tangible Impact}} of {{Gender Bias}} in {{Machine Translation}} with a {{Human-centered Study}}},
  shorttitle = {What the {{Harm}}?},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Savoldi, Beatrice and Papi, Sara and Negri, Matteo and {Guerberof-Arenas}, Ana and Bentivogli, Luisa},
  year = {2024},
  pages = {18048--18076},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1002},
  urldate = {2025-04-06},
  abstract = {Gender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from {$\sim$}90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/VIZLY5NZ/Savoldi et al. - 2024 - What the Harm Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-ce.pdf}
}

@article{schiebingerScientificResearchMust2014,
  title = {Scientific Research Must Take Gender into Account},
  author = {Schiebinger, Londa},
  year = {2014},
  month = mar,
  journal = {Nature},
  volume = {507},
  number = {7490},
  pages = {9--9},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/507009a},
  urldate = {2025-06-06},
  abstract = {From car design to drug discovery, the failure to acknowledge sex differences can be costly and even lethal, argues Londa Schiebinger.},
  copyright = {2014 Springer Nature Limited},
  langid = {english},
  keywords = {Ethics,Research management},
  file = {/Users/khali/Zotero/storage/3KYNRP4V/Schiebinger - 2014 - Scientific research must take gender into account.pdf}
}

@misc{schmitzGermanAllProfessors2022,
  title = {In {{German}}, All Professors Are Male},
  author = {Schmitz, Dominic},
  year = {2022},
  month = aug,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/yjuhc},
  urldate = {2025-06-06},
  abstract = {Findings of previous behavioural studies suggest that the semantic nature of what is called the ``masculine generic'' in Modern Standard German is not generic but biased towards a masculine reading. Such findings run counter the traditional assumption of masculine generic forms to be gender-neutral and are the cause of debates within and outside the linguistic community. The present paper aims to explore the semantics of masculine generics by implementing ideas of discriminative learning; an approach that thus far has not yet been used in this matter. If the present results account for a male bias of the masculine generic, findings by previous studies, which relied on very different methodological approaches, are confirmed. Indeed, it is found that masculine generics are semantically much more similar to masculine explicits than to feminine explicits. This supports the notion of a male bias in masculine generics.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/khali/Zotero/storage/GMSN5TLT/Schmitz - 2022 - In German, all professors are male.pdf}
}

@article{schryenWritingQualitativeLiterature2015,
  title = {Writing {{Qualitative IS Literature Reviews}}---{{Guidelines}} for {{Synthesis}}, {{Interpretation}}, and {{Guidance}} of {{Research}}},
  author = {Schryen, Guido},
  year = {2015},
  journal = {Communications of the Association for Information Systems},
  volume = {37},
  issn = {15293181},
  doi = {10.17705/1CAIS.03712},
  urldate = {2025-05-09},
  abstract = {The literature review is an established research genre in many academic disciplines, including the IS discipline. Although many scholars agree that systematic literature reviews should be rigorous, few instructional texts for compiling a solid literature review, at least with regard to the IS discipline, exist. In response to this shortage, in this tutorial, I provide practical guidance for both students and researchers in the IS community who want to methodologically conduct qualitative literature reviews. The tutorial differs from other instructional texts in two regards. First, in contrast to most textbooks, I cover not only searching and synthesizing the literature but also the challenging tasks of framing the literature review, interpreting research findings, and proposing research paths. Second, I draw on other texts that provide guidelines for writing literature reviews in the IS discipline but use many examples of published literature reviews. I use an integrated example of a literature review, which guides the reader through the overall process of compiling a literature review.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/JNZJRFZE/Schryen - 2015 - Writing Qualitative IS Literature Reviews‚ÄîGuidelines for Synthesis, Interpretation, and Guidance of.pdf}
}

@article{schwemmerDiagnosingGenderBias2020,
  title = {Diagnosing {{Gender Bias}} in {{Image Recognition Systems}}},
  author = {Schwemmer, Carsten and Knight, Carly and {Bello-Pardo}, Emily D. and Oklobdzija, Stan and Schoonvelde, Martijn and Lockhart, Jeffrey W.},
  year = {2020},
  month = jan,
  journal = {Socius},
  volume = {6},
  pages = {2378023120967171},
  publisher = {SAGE Publications},
  issn = {2378-0231},
  doi = {10.1177/2378023120967171},
  urldate = {2025-05-28},
  abstract = {Image recognition systems offer the promise to learn from images at scale without requiring expert knowledge. However, past research suggests that machine learning systems often produce biased output. In this article, we evaluate potential gender biases of commercial image recognition platforms using photographs of U.S. members of Congress and a large number of Twitter images posted by these politicians. Our crowdsourced validation shows that commercial image recognition systems can produce labels that are correct and biased at the same time as they selectively report a subset of many possible true labels. We find that images of women received three times more annotations related to physical appearance. Moreover, women in images are recognized at substantially lower rates in comparison with men. We discuss how encoded biases such as these affect the visibility of women, reinforce harmful gender stereotypes, and limit the validity of the insights that can be gathered from such data.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/8Y9SFYN6/Schwemmer et al. - 2020 - Diagnosing Gender Bias in Image Recognition Systems.pdf}
}

@article{sczesnyCanGenderFairLanguage2016,
  title = {Can {{Gender-Fair Language Reduce Gender Stereotyping}} and {{Discrimination}}?},
  author = {Sczesny, Sabine and Formanowicz, Magda and Moser, Franziska},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00025},
  urldate = {2025-05-16},
  abstract = {Gender-fair language (GFL) aims at reducing gender stereotyping and discrimination. Two principle strategies have been employed to make languages gender-fair and to treat women and men symmetrically: neutralization and feminization. Neutralization is achieved, for example, by replacing male-masculine forms (policeman) with genderunmarked forms (police officer), whereas feminization relies on the use of feminine forms to make female referents visible (i.e., the applicant... he or she instead of the applicant... he). By integrating research on (1) language structures, (2) language policies, and (3) individual language behavior, we provide a critical review of how GFL contributes to the reduction of gender stereotyping and discrimination. Our review provides a basis for future research and for scientifically based policy-making.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/69MY5UWT/Sczesny et al. - 2016 - Can Gender-Fair Language Reduce Gender Stereotyping and Discrimination.pdf}
}

@article{shresthaExploringGenderBiases2022,
  title = {Exploring Gender Biases in {{ML}} and {{AI}} Academic Research through Systematic Literature Review},
  author = {Shrestha, Sunny and Das, Sanchari},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  pages = {976838},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.976838},
  urldate = {2025-04-06},
  abstract = {Automated systems that implement Machine learning (ML) and Artificial Intelligence (AI) algorithms present promising solutions to a variety of technological and non-technological issues. Although, industry leaders are rapidly adopting these systems for anything from marketing to national defense operations, these systems are not without flaws. Recently, many of these systems are found to inherit and propagate gender and racial biases that disadvantages the minority population. In this paper, we analyze academic publications in the area of gender biases in ML and AI algorithms thus outlining different themes, mitigation and detection methods explored through research in this topic. Through a detailed analysis of               N               = 120 papers, we map the current research landscape on gender specific biases present in ML and AI assisted automated systems. We further point out the aspects of ML/AI gender biases research that are less explored and require more attention. Mainly we focus on the lack of user studies and inclusivity in this field of study. We also shed some light into the gender bias issue as experienced by the algorithm designers. In conclusion, in this paper we provide a holistic view of the breadth of studies conducted in the field of exploring, detecting and mitigating gender biases in ML and AI systems and, a future direction for the studies to take in order to provide a fair and accessible ML and AI systems to all users.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/WRGK82UA/Shrestha and Das - 2022 - Exploring gender biases in ML and AI academic research through systematic literature review.pdf}
}

@misc{skyquestMachineTranslationMT2025,
  title = {Machine {{Translation}} ({{MT}}) {{Market Size}}, {{Growth}} \& {{Trends Report}} {\textbar} 2032},
  author = {SkyQuest},
  year = {2025},
  urldate = {2025-05-23},
  abstract = {Machine Translation (MT) Market By Technology(Statistical Machine Translation (SMT), Neural Machine Translation (NMT)), By Application(Automotive, Healthcare), By Type of MT System(Rule-Based Machine Translation (RBMT), Example-Based Machine Translation (EBMT)) and By Region.},
  howpublished = {https://www.skyquestt.com/report/machine-translation-market},
  langid = {english},
  file = {/Users/khali/Zotero/storage/K5CTD5KM/machine-translation-market.html}
}

@incollection{smacchiaDoesAIReflect2024,
  title = {Does {{AI Reflect Human Behaviour}}? {{Exploring}} the {{Presence}} of {{Gender Bias}} in {{AI Translation Tools}}},
  shorttitle = {Does {{AI Reflect Human Behaviour}}?},
  booktitle = {Digital ({{Eco}}) {{Systems}} and {{Societal Challenges}}},
  author = {Smacchia, Marco and Za, Stefano and Arenas, Alvaro},
  editor = {Braccini, Alessio Maria and Ricciardi, Francesca and Virili, Francesco},
  year = {2024},
  volume = {72},
  pages = {355--373},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-75586-6_19},
  urldate = {2025-02-27},
  abstract = {Natural language processing tools are becoming more and more important in our daily life, enabling us to perform many tasks in a timely and efficient manner. However, as the utilisation of these tools growth, so does the risk of unexpected consequences due to the presence of bias. This study investigates the presence of gender bias within the most popular neural machine translation and large language model tools. We defined a set of Italian sentences concerning ten specific jobs, where the gender of the subjects is not explicitly mentioned. Employing those AI tools, we translated the sentences from Italian to English, requiring the gender to be explicitly mentioned. Afterwards, we developed a survey to obtain human translations for the same sentences, allowing us to compare the differences between the responses generated by the tools and those from individuals. Results show a high presence of gender bias especially for the jobs associated with a male gender and demonstrate a consistency between the outcome obtained by the tools and the results of the survey. These findings serve as a starting point for exploring the origins of gender bias within natural language processing tools and how they reflect gender distributions in our society and human behaviour regarding job occupations.},
  isbn = {978-3-031-75585-9 978-3-031-75586-6},
  langid = {english},
  file = {/Users/khali/Zotero/storage/DQXYL28J/Smacchia et al. - 2024 - Does AI Reflect Human Behaviour Exploring the Presence of Gender Bias in AI Translation Tools.pdf}
}

@article{soundararajanInvestigatingGenderBias2024,
  title = {Investigating {{Gender Bias}} in {{Large Language Models Through Text Generation}}},
  author = {Soundararajan, Shweta and Delany, Sarah Jane},
  year = {2024},
  journal = {Association for Computational Linguistics},
  volume = {Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)},
  pages = {410--424},
  abstract = {Large Language Models (LLMs) have swiftly become essential tools across diverse text generation applications. However, LLMs also raise significant ethical and societal concerns, particularly regarding potential gender biases in the text they produce. This study investigates the presence of gender bias in four LLMs: ChatGPT 3.5, ChatGPT 4, Llama 2 7B, and Llama 2 13B. By generating a gendered language dataset using these LLMs, focusing on sentences about men and women, we analyze the extent of gender bias in their outputs. Our evaluation is two-fold: we use the generated dataset to train a gender stereotype detection task and measure gender bias in the classifier, and we perform a comprehensive analysis of the LLM-generated text at both the sentence and word levels. Gender bias evaluations in classification tasks and lexical content reveal that all the LLMs demonstrate significant gender bias. ChatGPT 4 and Llama 2 13B exhibit the least gender bias, with weak associations between gendered adjectives used and the gender of the person described in the sentence. In contrast, ChatGPT 3.5 and Llama 2 7B exhibit the most gender bias, showing strong associations between the gendered adjectives used and the gender of the person described in the sentence.},
  langid = {english},
  file = {/Users/khali/Zotero/storage/7IN955FN/Soundararajan and Delany - Investigating Gender Bias in Large Language Models Through Text Generation.pdf}
}

@misc{stanczakSurveyGenderBias2021,
  title = {A {{Survey}} on {{Gender Bias}} in {{Natural Language Processing}}},
  author = {Stanczak, Karolina and Augenstein, Isabelle},
  year = {2021},
  month = dec,
  number = {arXiv:2112.14168},
  eprint = {2112.14168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.14168},
  urldate = {2025-05-13},
  abstract = {Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We see overcoming these limitations as a necessary development in future research. CCS Concepts: {$\bullet$} Computing methodologies {$\rightarrow$} Natural language processing; Machine Learning; {$\bullet$} Computing methodologies {$\rightarrow$} Language resources.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/khali/Zotero/storage/F9ZEUDCP/Stanczak and Augenstein - 2021 - A Survey on Gender Bias in Natural Language Processing.pdf}
}

@misc{stanovskyEvaluatingGenderBias2019,
  title = {Evaluating {{Gender Bias}} in {{Machine Translation}}},
  author = {Stanovsky, Gabriel and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2019},
  number = {arXiv:1906.00591},
  eprint = {1906.00591},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.00591},
  urldate = {2025-04-03},
  abstract = {We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., ``The doctor asked the nurse to help her in the operation''). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word ``doctor''). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to genderbiased translation errors for all tested target languages. Our data and code are publicly available at shorturl.at/dimuD.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/GNSTFCIL/Stanovsky et al. - 2019 - Evaluating Gender Bias in Machine Translation.pdf}
}

@misc{stellaDatasetStudyingGender2021,
  title = {A {{Dataset}} for {{Studying Gender Bias}} in {{Translation}}},
  author = {Stella, Romina and Austermann, Anja and Johnson, Melvin and Linch, Michelle and Niu, Mengmeng and Pushkarna, Mahima and Shah, Apu and Webster, Kellie},
  year = {2021},
  urldate = {2025-04-10},
  abstract = {Posted by Romina Stella, Product Manager, Google Translate Advances on neural machine translation (NMT) have enabled more natural and fluid transla...},
  howpublished = {https://research.google/blog/a-dataset-for-studying-gender-bias-in-translation/},
  langid = {english},
  file = {/Users/khali/Zotero/storage/4E6IHKL2/a-dataset-for-studying-gender-bias-in-translation.html}
}

@misc{thompsonShockingAmountWeb2024,
  title = {A {{Shocking Amount}} of the {{Web}} Is {{Machine Translated}}: {{Insights}} from {{Multi-Way Parallelism}}},
  shorttitle = {A {{Shocking Amount}} of the {{Web}} Is {{Machine Translated}}},
  author = {Thompson, Brian and Dhaliwal, Mehak Preet and Frisch, Peter and Domhan, Tobias and Federico, Marcello},
  year = {2024},
  month = jun,
  number = {arXiv:2401.05749},
  eprint = {2401.05749},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.05749},
  urldate = {2025-05-23},
  abstract = {We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/khali/Zotero/storage/6T4RYL32/Thompson et al. - 2024 - A Shocking Amount of the Web is Machine Translated Insights from Multi-Way Parallelism.pdf;/Users/khali/Zotero/storage/BP58I6ED/2401.html}
}

@article{tiedemannOPUSMTBuildingOpen2020,
  title = {{{OPUS-MT}} -- {{Building}} Open Translation Services for the {{World}}},
  author = {Tiedemann, Jorg and Thottingal, Santhosh},
  year = {2020},
  journal = {European Association for Machine Translation},
  volume = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
  pages = {479--480},
  langid = {english},
  file = {/Users/khali/Zotero/storage/63AA5VDK/Tiedemann and Thottingal - OPUS-MT ‚Äì Building open translation services for the World.pdf}
}

@incollection{ullmannGenderBiasMachine2022,
  title = {Gender {{Bias}} in {{Machine Translation Systems}}},
  booktitle = {Artificial {{Intelligence}} and {{Its Discontents}}},
  author = {Ullmann, Stefanie},
  editor = {Hanemaayer, Ariane},
  year = {2022},
  pages = {123--144},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-88615-8_7},
  urldate = {2025-05-16},
  isbn = {978-3-030-88614-1 978-3-030-88615-8},
  langid = {english},
  file = {/Users/khali/Zotero/storage/2CJDXUME/Ullmann - 2022 - Gender Bias in Machine Translation Systems.pdf}
}

@misc{unitednationsAchieveGenderEquality2023,
  title = {Achieve {{Gender Equality And Empower All Women}} and {{Girls}}},
  author = {{United Nations}},
  year = {2023},
  urldate = {2025-05-28},
  howpublished = {https://sdgs.un.org/goals/goal5},
  file = {/Users/khali/Zotero/storage/K8LZ2XM3/goal5.html}
}

@article{zotero-item-199,
  type = {Article}
}
