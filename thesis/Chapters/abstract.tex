\thispagestyle{empty}


\vspace*{1cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{1cm}

\noindent 
Gender bias in Englishâ€“German Machine Translation often appears in forms such as generic masculine defaulting and occupation stereotyping. These biases can perpetuate unequal representations and feed back into future translation models, reinforcing biased outputs in society. This thesis examines how accurately multilingual BERT (mBERT) can detect such bias. The model was fine-tuned on limited datasets with varying annotation quality, which caused its main limitations. The classifier occasionally (1) misclassifies German gender-fair language forms as biased, (2) fails to detect bias in political and government terms, (3) fails to recognize semantically gendered words as unbiased, (4) is sensitive to punctuation and capitalization, and (5) struggles with sentences that contain both neutral and gendered subjects. Despite these gaps, the model achieved an F1 score of 0.966 and proves effective for core bias cases. It reached 84.6\% accuracy on a small handcrafted evaluation dataset with practical sentences like job postings and edge cases. As an intermediary step, the work offers a trained model, sufficiently effective for practical bias detection, and an application that make biased translations visible while indicating areas for further investigation and improvement. The code is available at \url{https://github.com/phmkhali/bias-detector-en-de}.