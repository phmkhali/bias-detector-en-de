\appendix
\renewcommand{\thesection}{\Alph{section}}

\section{Datasets and Evaluation Tables}\label{appendix:datasets}

\subsection{Handcrafted Test Set Sentences}
\label{tab:handcrafted_testset}
\input{./Bilder/handcrafted_test_sentences.tex}

\subsection{Performance of Dataset Tuning Test Runs}
\label{appendix:dataset_tuning_table}
\input{./Bilder/dataset_tuning_table.tex}

\subsection{False Positives and False Negatives from Held-out Test Set}
\label{tab:fp_fn_table}
\input{./Bilder/fp_fn_table.tex}

\subsection{Handcrafted Test Set Results}
\label{tab:handcrafted_testset_results}
\input{./Bilder/handcrafted_test_sentences_results.tex}


% -------------
\section{Use of Artificial Intelligence}\label{appendix:artificial_intelligece}

\subsection{Perplexity.ai for Literature Research}
Used Perplexity.ai to find additional sources and references on gender bias in EN-DE MT.

\includepdf[pages=-]{./Literatur/Key Sources on Gender Bias in Machine Translation 21-may-2025.pdf}

\subsection{Gemini for Synthetic Data Generation}\label{appendix:gemini_prompt}
Prompting Gemini to generate full sentences from the existing Building Bridges dataset \parencite{lardelliBuildingBridgesDataset2024}, which contained only nouns. I provided the lardelli\_singular.csv and lardelli\_plural.csv files as input. Manual sentence creation would have been too resource intensive. GPT and Deepseek were also tested, but they did not handle large amounts of data efficiently and only produced output in small batches. The final output was a singular output.csv, which I subsequently reviewed and corrected through additional manual rounds to fix errors.
\subsubsection{Prompt: } 

\begin{lstlisting}
Your Role: You are an expert linguist and data generation specialist. Your task is to create a high-quality structured dataset for ML by generating English sentences and their German translations based on two CSV files.

Goal: Process lardelli_singular.csv and lardelli_plural.csv to produce a final CSV with columns: english,german,label. Generate multiple variations per English sentence using different German translation rules.

Input Data:
lardelli_singular.csv: ID,German_MS,German_FS,Gender-Neutral,Gender-Inclusive,English,English_P
lardelli_plural.csv: ID,German_MP,German_FP,Gender-Neutral,Gender-Inclusive,English

Instructions:
1. Iterate through lardelli_singular.csv.
2. Generate 3-5 simple, correct English sentences for the singular noun (English) and plural noun (English_P). Create singular and plural versions.
Example: "The deputy is having a great day." / "The deputies are having a great day."
3. Generate German translations:
  - Singular: use current row.
    - Label 1: German_MS, German_FS
    - Label 0: Gender-Neutral, Gender-Inclusive
  - Plural: match ID in lardelli_plural.csv
    - Label 1: German_MP, German_FP
    - Label 0: Gender-Neutral, Gender-Inclusive
4. Quality rules:
  - Ensure grammatical correctness (articles, adjectives, verbs, case, number).
  - Skip missing values.
  - Generate all possible variants for balance.
  - English sentences should be varied.
  - Final CSV: english,german,label (use quotes if commas appear).

Example Output:
english,german,label
"The deputy is having a great day","Der Stellvertreter hat einen tollen Tag",1
"The deputy is having a great day","Die Stellvertreterin hat einen tollen Tag",1
"The deputy is having a great day","die Stellvertretung hat einen tollen Tag",0
"The deputy is having a great day","der*die Stellvertreter*in hat einen tollen Tag",0
"The deputies are having a great day","Die Stellvertreter haben einen tollen Tag",1
"The deputies are having a great day","Die Stellvertreterinnen haben einen tollen Tag",1
"The deputies are having a great day","Die Stellvertretung hat einen tollen Tag",0
"The deputies are having a great day","die Stellvertreter*innen haben einen tollen Tag",0

Begin generation now. If output is too large, produce in batches.

\end{lstlisting}

\subsubsection{Output: } 
\begin{lstlisting}
The data generation is complete, and the output has been saved to output_translations.csv. Overview of the process:

Loaded Data: The lardelli_singular.csv and lardelli_plural.csv files were loaded into DataFrames.

Sentence Generation: For each noun in lardelli_singular.csv, 3 singular and 3 plural English sentences were generated using simple, grammatically correct templates.

German Translation Generation: For each singular English sentence, corresponding German translations were generated using German_MS, German_FS, Gender-Neutral, and Gender-Inclusive columns from lardelli_singular.csv. For each plural English sentence, matching rows were found in lardelli_plural.csv using the ID, and German translations were generated using German_MP, German_FP, Gender-Neutral, and Gender-Inclusive columns. Translations were labeled as 1 (gendered) or 0 (neutral/inclusive). Rows with missing German translation values were skipped.

Output: All generated sentences and their translations were compiled into a single CSV file named output_translations.csv with columns english,german,label. Please find the generated CSV file below.

\end{lstlisting}

\subsection{Use of AI for Code Generation}\label{appendix:ai_code}
During the development of the practical part, I used ChatGPT and DeepSeek to assist in generating portions of the implementation for the demo, specifically in \texttt{app.py}, \texttt{utils.py}, and \texttt{translate.py}. The process involved multiple rounds of suggestions and refinements. All AI-generated code was reviewed, adapted, and integrated to ensure correctness and suitability for the project. AI assistance supported implementation but did not replace my own programming work or design decisions. Below is one example of AI-assisted code generation. Due to the iterative nature of the process, not all intermediate outputs can be shown. This example illustrates how AI was used to support the implementation.


\subsubsection{Prompt: } 

\begin{lstlisting}
look at my app.py: 
import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from utils import split_sentences, predict_bias_batch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "./model_output" 
MAX_LENGTH = 128
BIAS_CONF_THRESHOLD = 0.9

@st.cache_resource(show_spinner=False)
def load_model_and_tokenizer():
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
    model.to(device)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model_and_tokenizer()

st.title("English to German Translation with Gender Bias Detection")
text = st.text_area("Enter English text here:")

if st.button("Translate"):
    if not text.strip():
        st.write("Please enter some text")
    else:
        st.write("### Results")
        sentences = split_sentences(text)
        results = predict_bias_batch(tokenizer, model, sentences, max_length=MAX_LENGTH, device=device, bias_threshold=BIAS_CONF_THRESHOLD)

        for sentence, translation, pred, conf in results:
            st.write(f"**english:** {sentence}")
            st.write(f"**german:** {translation}")
            if pred == 1 and conf >= BIAS_CONF_THRESHOLD:
                st.warning(f"Bias detected confidence: {conf:.2f}")
            else:
                st.success(f"No bias detected confidence: {conf:.2f}")
            st.markdown("---")

and utils.py:

import re
from translate import translate
import torch

def split_sentences(text):
    # split on . ! or ? followed by space(s)
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return [s for s in sentences if s]

def predict_bias_batch(tokenizer, model, sentences, max_length=128, device='cpu', bias_threshold=0.9):
    results = []
    for sentence in sentences:
        translation = translate(sentence)
        inputs = tokenizer(
            sentence,
            translation,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=max_length,
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)
            pred = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred].item()
        results.append((sentence, translation, pred, confidence))
    return results

right now there is only one input field that utilises the translate script. i want another option that the user can choose to manually input translation, for example a tab selection. there are two input fields where the user inputs english and german and click a button "check bias". then the same standard bias detection script runs. you may add a new file or change existing ones
\end{lstlisting}

\subsubsection{Output:}
\begin{lstlisting}
To add the option for manual translation input, use Streamlit's `st.tabs` to create two modes:

1. **Translate Mode** (uses your `translate()` function).
2. **Manual Input Mode** (user provides both English and German text).

You only need to slightly update `app.py` and optionally refactor `predict_bias_batch` to handle one or more pairs directly. You do **not** need to change `utils.py` unless you want cleaner logic separation.

import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from utils import split_sentences, predict_bias_batch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "./model_output"
MAX_LENGTH = 128
BIAS_CONF_THRESHOLD = 0.9

@st.cache_resource(show_spinner=False)
def load_model_and_tokenizer():
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
    model.to(device)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model_and_tokenizer()

st.title("Gender Bias Detection in English-German Translations")

tab1, tab2 = st.tabs(["Translate", "Manual Input"])

with tab1:
    text = st.text_area("Enter English text here:")
    if st.button("Translate and Check Bias"):
        if not text.strip():
            st.write("Please enter some text")
        else:
            st.write("### Results")
            sentences = split_sentences(text)
            results = predict_bias_batch(
                tokenizer,
                model,
                sentences,
                max_length=MAX_LENGTH,
                device=device,
                bias_threshold=BIAS_CONF_THRESHOLD,
                use_translation=True,
            )
            for sentence, translation, pred, conf in results:
                st.write(f"**english:** {sentence}")
                st.write(f"**german:** {translation}")
                if pred == 1 and conf >= BIAS_CONF_THRESHOLD:
                    st.warning(f"Bias detected confidence: {conf:.2f}")
                else:
                    st.success(f"No bias detected confidence: {conf:.2f}")
                st.markdown("---")

with tab2:
    manual_en = st.text_area("Enter English sentence:")
    manual_de = st.text_area("Enter German translation:")
    if st.button("Check Bias"):
        if not manual_en.strip() or not manual_de.strip():
            st.write("Please enter both English and German text")
        else:
            result = predict_bias_batch(
                tokenizer,
                model,
                [(manual_en, manual_de)],
                max_length=MAX_LENGTH,
                device=device,
                bias_threshold=BIAS_CONF_THRESHOLD,
                use_translation=False,
            )[0]
            sentence, translation, pred, conf = result
            st.write(f"**english:** {sentence}")
            st.write(f"**german:** {translation}")
            if pred == 1 and conf >= BIAS_CONF_THRESHOLD:
                st.warning(f"Bias detected confidence: {conf:.2f}")
            else:
                st.success(f"No bias detected confidence: {conf:.2f}")
\end{lstlisting}
