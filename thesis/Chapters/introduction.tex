\chapter{Introduction}
    Modern Machine Translation (MT) tools have made cross-lingual communication more accessible than ever. Services like Google Translate, used by over 200 million people daily \parencite{pratesAssessingGenderBias2019,shresthaExploringGenderBiases2022}, offer fast and often accurate translations of full sentences and longer texts. They are widely used in daily life as well as in fields such as healthcare, law, and business \parencite{kapplAreAllSpanish2025}.

    The MT market continues to grow rapidly. A recent report by \textcite{skyquestMachineTranslationMT2025} valued it at 980 million USD in 2023, with projections reaching 2.78 billion USD in the coming years. As the use of MT systems expands, their output has a growing impact on how users access and understand information, raising concerns about the quality and broader implications of these translations.
    
    MT systems are trained on large-scale text corpora that often contain implicit social biases. Translation models tend to reproduce such patterns rather than correct them. One widely studied phenomenon is gender bias, which has been observed across different systems and language pairs \parencite{choMeasuringGenderBias2019,soundararajanInvestigatingGenderBias2024,smacchiaDoesAIReflect2024}. 

    A common example is the use of gendered terms in the translation of gender-neutral sentences. The English input “The nurse is hard-working” does not specify gender, yet the German output may be “Die Krankenschwester ist fleißig,” which assumes a female identity. In contrast, “The surgeon is hard-working” may be translated as “Der Chirurg ist fleißig,” implying a male identity. 
    
    Such translations introduce gendered information that is absent from the source text, reflecting implicit assumptions about certain roles. When these assumptions appear repeatedly in contexts like job descriptions or media reporting, they risk reinforcing societal stereotypes. This contradicts international standards set by organizations like the United Nations, UNESCO, and the European Union, which highlight inclusive language as essential for achieving the Sustainable Development Goals by 2030 \parencite{sczesnyCanGenderFairLanguage2016,unitednationsAchieveGenderEquality2023}.

\section{Motivation}
    The practical handling of such bias remains limited. Existing research has largely focused on measuring overall bias by counting gendered outputs, comparing them to expected patterns, or testing models on standard benchmarks \parencite{rescignoGenderBiasMachine2023,barclayInvestigatingMarkersDrivers2024a,pratesAssessingGenderBias2019,savoldiWhatHarmQuantifying2024}. These studies provide insights into how often and in what forms bias appears, but they do not offer solutions for detecting biased translations as they occur.

    The lack of established tools for real-time detection of gender bias is not a challenge unique to MT. In other domains, such as computer vision or automated hiring, studies have exposed similar patterns. Image recognition systems, for example, have labeled portraits of women with terms like “girl,” “cake,” or “kitchen,” while men received occupational labels like “attorney” or “executive” \parencite{schwemmerDiagnosingGenderBias2020}. As long as biased translations remain hidden in individual outputs, they are likely to go unnoticed and unchallenged, underlining the need for methods making them visible to users.

\section{Research Questions}
  Building on these challenges, this thesis aims to develop a system for detecting gender bias in English-to-German (EN-DE) translations using Natural Language Processing (NLP). The main goal is to create a transparent and efficient classifier that flags biased outputs from MT. For this, a multilingual BERT model (mBERT), which processes text in multiple languages, will be manually fine-tuned for the task. The classifier will be integrated into a demo application that allows users to test bias detection in real time.

    The focus on EN-DE was chosen because it is widely used in research, supported by numerous open datasets and high-quality models. Moreover, structural differences between English and German make gender bias more visible in translation. While English uses minimal gender marking, German often requires explicit gender assignment.

    The following research questions structure the evaluation:

    \begin{enumerate}[label=\textbf{RQ\arabic*:}]
    \item How accurately does fine-tuning mBERT detect gender bias in EN-DE translations?
    \item What limitations arise in this detection system?
    \end{enumerate}

    In line with similar studies in MT bias (e.g. \textcite{smacchiaDoesAIReflect2024}, \textcite{rescignoGenderBiasMachine2023}), RQ1 examines classifier performance using standard evaluation metrics. Overall accuracy on a held-out test set will be reported, along with precision and recall for the biased class and the F1-score to balance these measures. 
    
    RQ2 investigates the detection system's limitations in realistic settings by analyzing how ambiguous contexts (e.g., unclear pronouns, subtle gender cues) and grammatical gender (in German) impact classification accuracy. It groups and quantifies errors (false positives/negatives) to identify common failure modes. Additionally, the system's generalization ability is evaluated on out-of-domain texts.

\section{Overview of Chapters}
\textbf{WRITE AFTER IMPLEMENTATION PART}
