\chapter{Introduction}
Machine Translation (MT) is a sub-field of computational linguistic that uses computer software to translate texts between languages \citep{linMachineTranslationAcademic2009}. It is a major area within Natural Language Processing (NLP), a branch of Artificial Intelligence (AI) \citep{smacchiaDoesAIReflect2024}. This technology helps millions of people communicate across languages, whether in every situations or high-stakes domains like healthcare, law and business \citep{kapplAreAllSpanish2025}. Users rely on it to translate everything from casual conversations to medical prescriptions and legal documents. Tools like Google Translate serve over 200 million users daily \citep{pratesAssessingGenderBias2019,shresthaExploringGenderBiases2022}, with new advanced translation models appearing on the market frequently. According to a market analysis by \citet{skyquestMachineTranslationMT2025}, the MT market size was valued at 980 million USD in 2023 and is projected to reach 2.78 billion USD by 2023. 

With this growing availability and accessibility of free MT tools capable of handling complex sentences, their use in translating large volumes of online content is increasing \citep{thompsonShockingAmountWeb2024}. This not only expands their influence on global access to information, but also shapes how readers perceive and interpret that content. Automated and unsupervised translations raise new concerns: not just about quality, but also about bias. One aspect is gender bias. Several studies \citep{smacchiaDoesAIReflect2024,choMeasuringGenderBias2019,stanczakSurveyGenderBias2021,soundararajanInvestigatingGenderBias2024} confirm that MT systems trained on large-scale datasets that incorporate societal biases, can learn and perpetuate gender biases present in the training data. In short, if the training data reflects gender stereotypes, the translation system is likely to repeat them. This includes, but is not limited to, the insertion of gendered pronouns in place of originally gender-neutral terms, and the use of stereotypically gendered occupational titles.

There is a risk of incorrect gender assignment when translating between languages with and without grammatical gender. For example, the gender-neutral English sentences "The surgeon is hard-working" and "The nurse is hard-working" are translated into German as "Der Chirurg ist fleißig" and "Die Krankenschwester ist fleißig" respectively, as seen in \autoref{fig:gt_surgeon_example} and \autoref{fig:deepL_surgeon_example}. These translations introduce occupational gender stereotypes. “Der Chirurg” is the masculine form of “surgeon” in German, adding a male gender where the original English sentence did not specify one. Similarly, “Die Krankenschwester” is the feminine form of “nurse,” again assigning a gender that was not present in the source. These patterns are not just technical flaws. They can reinforce harmful stereotypes in real-world contexts. The following section outlines the broader motivation behind this thesis.


\section{Motivation}

Academia has come to the consensus that MT systems do default to male pronouns when gender in the source sentence is ambiguous. In addition, as shown in the earlier example where "the surgeon" and "the nurse" were translated with stereotypical genders, the reinforcement of occupational stereotypes is an increasing concern. When MT is used for job descriptions, recommendation letters, or resumes and it inserts or reinforces unfairly gendered language \citep{bolukbasiManComputerProgrammer2016}, it may discourage individuals whose gender is misrepresented or stereotyped. In turn, this would reduce their chances of success in recruitment processes. 
This may also bring broader consequences for businesses: Failing to address this issue can lead to the exclusion of qualified candidates, reduce diversity and contradict international standards. Organizations like the United Nations, UNESCO, and the European Union stress the importance of gender equality and inclusive language, making gender equality one of the 17 Sustainable Development Goals for 2030 \citep{sczesnyCanGenderFairLanguage2016,unitednationsAchieveGenderEquality2023}.
Ethical AI guidelines from global institutions also highlight the need for fair outcomes \citep{ullmannGenderBiasMachine2022}, therefore businesses need to meet these standards if they want to stay credible and act responsibly. 

Current research on this topic tends to focus more on the quantitative measurement of gender bias \citep{rescignoGenderBiasMachine2023,barclayInvestigatingMarkersDrivers2024a,smacchiaDoesAIReflect2024}, e.g. counting the occurences of gendered pronouns or grammatical forms in outputs when prompting models with a neutral input. It is then often compared against a standard or desired outcome like real-world demographic distributions \citep{smacchiaDoesAIReflect2024,pratesAssessingGenderBias2019} or human evaluation \citep{lardelliBuildingBridgesDataset2024,savoldiWhatHarmQuantifying2024}. However, current evaluations are not enough for accountability. Few approaches address an active gender bias detection layer. While this gap remains in translation systems, similar issues have been addressed in other domains. For example, as summarized by \citet{shresthaExploringGenderBiases2022}, \citet{schwemmerDiagnosingGenderBias2020} propose a detection framework to uncover gender bias in facial recognition technologies. Their findings show that these systems are more accurate in identifying individuals as women when the images conform to stereotypical feminine features like long hair or makeup. In some cases, systems even associated such images with stereotypically gendered labels like "kitchen" or "cake," despite these elements not being present. 
A detection system specifically for MT would increase linguistic transparency, because without the development of bias-aware tools, problematic translations are likely to scale without oversight. Therefore, addressing gender bias in MT becomes both a social and ethical necessity.

\section{Problem Statement and Research Questions}

The core problem boils down to the significant bias towards the masculine form in English-German MTs, sometimes consituting 93-96\% of translations for isolated words \citep{lardelliBuildingBridgesDataset2024}. These outputs often reflect social stereotypes rather than objective translations, yet current systems offer no mechanism to detect or signal when such bias occurs \citep{rescignoGenderBiasMachine2023}. To address this, this thesis deploys a blackbox approach to explore how fine-tuning a pre-trained multilingual BERT model can help detect gender bias in MT outputs. The model takes an input sentence and its corresponding German translation and predicts whether the translation introduces gender bias. It focuses on identifying two common cases: added gendered pronouns and wrongly gendered nouns.

The translation system used is \href{https://github.com/Helsinki-NLP/Opus-MT?tab=readme-ov-file}{Opus-MT}, an open-source neural MT model. %mention which bert i will use% 
Translations are passed through BERT, trained on a dataset I have constructed by combining and adapting several existing datasets from other researchers. The classifier is lightweight and efficient, aiming for transparent behavior and easy integration into other tools \citep{devlinBERTPretrainingDeep2019}. Its predictions are used to highlight biased parts in a web-based demo. The goal is not to build a perfect detector, but a working proof of concept that shows how bias can be flagged automatically. This supports more critical use of MT systems and encourages further development of bias-aware translation tools.

The main research question is: \textbf{How can a NLP-based binary classification model detect gender bias in English-German translations?}. This involves building a suitable training dataset, selecting features that capture bias patterns, and evaluating how well the model generalizes across different domains.

\section{Scope and Limitations}

This thesis focuses only on English-to-German machine translation due to my fluency in both languages, allowing me to evaluate the outputs and datasets directly. Extending the work to other language pairs would require native-level understanding to reliably identify subtle gender patterns and translation errors, which is beyond the current scope. More generally, gender bias in language is a complex issue that goes far beyond simple word associations. It becomes especially difficult to detect when sentences contain multiple subjects, indirect references, or ambiguous pronouns. For example, as \citet{barclayInvestigatingMarkersDrivers2024a} explain, the sentence “He went to see her mother” clearly implies three people, while “He went to see his mother” could refer to either two or three. These types of structures introduce ambiguity that makes annotation and evaluation much harder. Creating a dataset that captures such linguistic complexity would require significant effort and careful control of variables. One broader limitation in building datasets for complex scenarios with multiple subjects is the difficulty of isolating the influence of each gendered entity \citep{lardelliBuildingBridgesDataset2024}. When working with natural language sources, it becomes hard to tell what caused the bias in the translation. Because of this, the focus of this thesis is on simpler sentence structures with a single subject. This makes it easier to identify and explain bias patterns. It also fits the intended use case: translating business texts like job advertisements or reports, which rarely involve multiple nested clauses or ambiguous pronouns.
 
\section{Overview of Chapters}
