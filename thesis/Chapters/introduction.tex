\chapter{Introduction}
    Modern Machine Translation (MT) tools have made cross-lingual communication more accessible than ever. Services like Google Translate, used by over 200 million people daily \parencite{pratesAssessingGenderBias2019,shresthaExploringGenderBiases2022}, offer fast and often accurate translations of full sentences and longer texts. MT is a rapidly growing market, with applications in daily life as well as in fields such as healthcare, law, and business \parencite{kapplAreAllSpanish2025}. A recent report by \textcite{skyquestMachineTranslationMT2025} valued it as a 980 million USD industry in 2023, with projections reaching 2.78 billion USD in the coming years. As the use of MT systems expands, their output has a growing impact on how users access and understand information, raising concerns about the quality and broader implications of these translations.
    
    MT systems are trained on large amounts of text that often contain implicit social biases. Translation models tend to reproduce those rather than correct them. One phenomenon is gender bias, which has been observed across different systems and language pairs \parencite{choMeasuringGenderBias2019,soundararajanInvestigatingGenderBias2024,smacchiaDoesAIReflect2024}. A common example is the use of gendered terms in the translation of gender-neutral sentences. The English input “The nurse is hard-working” does not specify gender, yet the German output may be “Die Krankenschwester ist fleißig,” which assumes a female identity. In contrast, “The surgeon is hard-working” may be translated as “Der Chirurg ist fleißig,” implying a male identity. Such translations introduce gendered information that is absent from the source text, reflecting implicit assumptions about certain roles. When these assumptions appear repeatedly in contexts like job descriptions or media reporting, they risk reinforcing societal stereotypes. This contradicts international standards set by organizations like the United Nations, UNESCO, and the European Union, which highlight inclusive language as essential for achieving the Sustainable Development Goals by 2030 \parencite{sczesnyCanGenderFairLanguage2016,unitednationsAchieveGenderEquality2023}.

\section{Motivation and Research Question}
    The practical handling of such bias remains limited. Existing research has largely focused on measuring overall bias by counting gendered outputs, comparing them to expected patterns, or testing models on standard benchmarks \parencite{rescignoGenderBiasMachine2023,barclayInvestigatingMarkersDrivers2024a,pratesAssessingGenderBias2019,savoldiWhatHarmQuantifying2024}. These studies provide insights into how often and in what forms bias appears, but they do not offer solutions for detecting biased translations as they occur. The lack of established tools for real-time detection of gender bias is also not a challenge unique to MT; similar issues appear in domains like computer vision and automated hiring \parencite{schwemmerDiagnosingGenderBias2020}. As long as biases remain hidden in individual outputs, they are likely to go unnoticed and unchallenged, underlining the need for methods making them visible to users. This thesis addresses these challenges by developing a system for detecting gender bias in English-to-German (EN-DE) translations using Natural Language Processing (NLP). The system is based on a multilingual BERT model (mBERT), which is manually fine-tuned for this task. The outcome is a transparent and efficient classifier that flags biased outputs from MT and is integrated into a demo application, allowing users to check translations for bias in real time. The EN–DE language pair was chosen due to the structural differences between English and German, which make gender bias more visible in translation. It is also supported by several public datasets and prior research. The evaluation is guided by the following research question:

    \vspace{0.8em}
    \noindent \textbf{RQ: How accurately does mBERT detect gender bias in EN-DE translations?}

    \vspace{0.8em}

    \noindent To answer this question, datasets from existing literature will be used for training and evaluation. These datasets, however, have known limitations. To address these, a second, hand-crafted dataset will be created to support model selection and evaluation. This approach aims to assess mBERT’s performance in detecting gender bias using standard metrics such as accuracy and F1. It also examines how well the model generalizes to unseen data and what limitations result from the system’s design.

\section{Overview of Chapters}
    First, key concepts are clarified and the fundamentals of BERT are explained to establish the theoretical foundation. The review of related work on bias in EN-DE translations then examines existing approaches and points out what they do not cover. The methodology for the bias detector is presented next, detailing dataset handling, training, evaluation, and application design. Implementation details follow, including the code structure and guidance for reproducing the work. The application is subsequently evaluated using the defined datasets and metrics, and the results are summarized and discussed in context. The thesis concludes with an outlook, exploring possible developments and directions for future research.