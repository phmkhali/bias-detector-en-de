\chapter{Introduction}
    Modern Machine Translation (MT) tools have made cross-lingual communication more accessible than ever. Services like Google Translate, used by over 200 million people daily \parencite{pratesAssessingGenderBias2019,shresthaExploringGenderBiases2022}, offer fast and often accurate translations of full sentences and even longer texts. They are widely used in daily life as well as in fields such as healthcare, law, and business \parencite{kapplAreAllSpanish2025}.

    The MT market continues to grow rapidly. A recent report by \textcite{skyquestMachineTranslationMT2025} valued it at 980 million USD in 2023, with projections reaching 2.78 billion USD in the coming years. As the use of MT systems expands, their output has a growing impact on how users access and understand information, raising concerns about the quality and broader implications of these translations.
    
    MT systems are trained on large-scale text corpora that often contain implicit social biases. Translation models tend to reproduce such patterns rather than correct them. One widely studied phenomenon is gender bias, which has been observed across different systems and language pairs \parencite{choMeasuringGenderBias2019,soundararajanInvestigatingGenderBias2024,smacchiaDoesAIReflect2024}. 

    A common example is the use of gendered terms in the translation of gender-neutral sentences. The English input “The nurse is hard-working” does not specify gender, yet the German output may be “Die Krankenschwester ist fleißig,” which assumes a female identity. In contrast, “The surgeon is hard-working” may be translated as “Der Chirurg ist fleißig,” implying a male identity. 
    
    These translations introduce gendered information that was absent from the source text. When repeated across domains like job descriptions or media reporting, such patterns can reinforce societal stereotypes. This contradicts international standards set by organizations like the United Nations, UNESCO, and the European Union, which highlight inclusive language as essential for achieving the Sustainable Development Goals by 2030 \parencite{sczesnyCanGenderFairLanguage2016,unitednationsAchieveGenderEquality2023}.

\section{Motivation}
    The practical handling of such bias remains limited. Existing research has largely focused on measuring overall bias by counting gendered outputs, comparing them to expected patterns, or testing models on standard benchmarks \parencite{rescignoGenderBiasMachine2023,barclayInvestigatingMarkersDrivers2024a,pratesAssessingGenderBias2019,savoldiWhatHarmQuantifying2024}. These studies provide insights into how often and in what forms bias appears, but they do not offer solutions for detecting biased translations as they occur.

    The lack of established tools for real-time detection of gender bias is not a challenge unique to MT. In other domains, such as computer vision or automated hiring, studies have exposed similar patterns. Image recognition systems, for example, have labeled portraits of women with terms like “girl,” “cake,” or “kitchen,” while men received occupational labels like “attorney” or “executive” \parencite{schwemmerDiagnosingGenderBias2020}. As long as biased translations remain hidden in individual outputs, they are likely to go unnoticed and unchallenged, underlining the need for methods making them visible to users.

\section{Research Questions}
  Building on these challenges, this thesis aims to develop a system for detecting gender bias in English-to-German (EN-DE) translations using Natural Language Processing (NLP). The main goal is to create a transparent and efficient classifier that flags potentially biased outputs from MT. For this, a multilingual BERT model (mBERT), which processes text in multiple languages, will be manually fine-tuned for the task. The classifier will be integrated into a demo application that allows users to test bias detection in real time.

    The focus on EN-DE was chosen because it is widely used in research, supported by numerous open datasets and high-quality models. Moreover, the linguistic differences between English and German highlight gender bias more clearly: English uses limited gender marking, while German requires gender assignment.

    The following research questions structure the evaluation:

    \begin{enumerate}[label=\textbf{RQ\arabic*:}]
    \item How accurately does fine-tuning mBERT detect gender bias in EN-DE translations?
    \item What limitations arise in this detection system?
    \end{enumerate}

    In line with similar studies in MT bias (e.g. \textcite{smacchiaDoesAIReflect2024}, \textcite{rescignoGenderBiasMachine2023}), RQ1 examines classifier performance using standard evaluation metrics. Overall accuracy on a held-out test set will be reported, along with precision and recall for the biased class and the F1-score to balance these measures. 
    
    RQ2 examines the main limitations of the detection system in realistic settings. First, ambiguous contexts such as unclear pronoun references and subtle gender cues will be tested to assess how they affect model decisions. Second, errors arising from grammatical gender in German, including incorrect noun-adjective agreement, will be analyzed. Third, the system’s ability to generalize will be evaluated on out-of-domain texts. Finally, an error analysis will identify the most common failure modes and quantify their frequency in terms of false positives and false negatives.

\section{Overview of Chapters}
\textbf{WRITE AFTER IMPLEMENTATION PART}
