\chapter{Introduction}
Machine Translation (MT) helps millions of people communicate across languages, in daily life and in areas like healthcare, law, and business \citep{kapplAreAllSpanish2025}. Services like Google Translate handle over 200 million users every day \citep{pratesAssessingGenderBias2019,shresthaExploringGenderBiases2022}. It is a fast-growing market. A report by \citet{skyquestMachineTranslationMT2025} valued it at 980 million USD in 2023, with projections reaching 2.78 billion USD. New and more advanced translation models keep appearing, and many of them are free to use. As a result, MT tools are now used to translate large volumes of content across domains.

With this widespread use, the output of MT systems increasingly shapes how people receive and interpret information. But automatic translations are not neutral. There is growing concern about the social effects of biased translations. One key issue is gender bias. MT systems are often trained on large datasets that reflect social norms and stereotypes. If the data contains gender bias, the system will likely reproduce it \citep{choMeasuringGenderBias2019,soundararajanInvestigatingGenderBias2024,smacchiaDoesAIReflect2024}.

A common case is the use of gendered terms in translations of gender-neutral input. For example, the English sentence “The nurse is hard-working” does not say anything about gender. But a translation system may render it in German as “Die Krankenschwester ist fleißig,” which uses the explicitly feminine term \textit{Krankenschwester}. Similarly, “The surgeon is hard-working” may become “Der Chirurg ist fleißig,” using the masculine form \textit{Chirurg}. These choices add gendered assumptions that were not present in the original. Such patterns are not just technical side effects. They can reinforce stereotypes, especially when they appear in job ads, reports, or other public texts.


\section{Motivation}

\subsection{Social and Ethical Importance of Addressing Gender Bias}\label{section:social_and_ethical_importance_of_addressing}
Academia has come to the consensus that MT systems do default to male pronouns when gender in the source sentence is ambiguous \citep{pratesAssessingGenderBias2019,choMeasuringGenderBias2019,rescignoGenderBiasMachine2023}.  In addition, translations often reflect traditional roles, like associating “nurse” with women and “surgeon” with men. This can affect people’s perceptions of jobs and reinforce gender roles.

When used in formal contexts like job descriptions or reference letters, biased translations can shape how a candidate is perceived. If a system always assigns male pronouns to leadership roles and female terms to caregiving roles, it may disadvantage those who do not match those stereotypes \citep{bolukbasiManComputerProgrammer2016}. This is not just a personal issue. It can reduce diversity and go against international standards. Organizations like the United Nations, UNESCO, and the European Union stress the importance of gender equality and inclusive language, making gender equality one of the 17 Sustainable Development Goals for 2030 \citep{sczesnyCanGenderFairLanguage2016,unitednationsAchieveGenderEquality2023}. 

Language also shapes thought. Research shows that readers often interpret masculine forms as male-specific, even if they are supposed to be generic \citep{sczesnyCanGenderFairLanguage2016}. Inclusive forms are more common in official documents, less so in everyday language. However, exposure matters. Frequent use of fair language makes it feel more normal. Detecting and addressing bias in MT can support this shift.

\subsection{Why Detection Systems Are Needed}

Current research on this topic tends to focus more on the quantitative measurement of gender bias \citep{rescignoGenderBiasMachine2023,barclayInvestigatingMarkersDrivers2024a,smacchiaDoesAIReflect2024}. Common methods include counting gendered forms in outputs and comparing them to demographic baselines or human expectations \citep{rescignoGenderBiasMachine2023,pratesAssessingGenderBias2019,savoldiWhatHarmQuantifying2024}. These are useful, but they do not help users identify specific biased translations in real-time. Evaluations are not enough for accountability. 

Other domains, like facial recognition, have already seen progress in active bias detection. For example, \citet{schwemmerDiagnosingGenderBias2020} showed that systems tend to label women more accurately if they match stereotypical appearances (e.g., long hair). Some models even linked female images to words like “kitchen” or “cake” based on bias patterns in training data. For MT, a detection layer is still missing. Without such tools, biased translations are likely to spread unnoticed. A detection system could flag potential bias in real time, improving transparency and encouraging more careful use.

\section{Problem Statement and Research Questions}
\textbf{DRAFT NEED TO REWRITE AFTER IMPLEMENTATION}
This thesis focuses on gender bias in English-to-German (EN-DE) MT. This language pair is widely used in research, with many open datasets and high-quality models available. It also involves a grammatical shift: English has limited gender marking, while German assigns gender to many nouns and pronouns. This structural difference makes gender bias more visible and easier to study in the translation outputs.

The core problem boils down to the significant bias towards the masculine form in EN-DE MTs, sometimes consituting 93-96\% of translations for isolated words \citep{lardelliBuildingBridgesDataset2024}. These outputs often reflect social stereotypes rather than objective translations, yet current systems offer no mechanism to detect or signal when such bias occurs \citep{rescignoGenderBiasMachine2023}. To address this, this thesis deploys a blackbox approach to explore how fine-tuning a pre-trained multilingual BERT model can help detect gender bias in MT outputs. The model takes an input sentence and its corresponding German translation and predicts whether the translation introduces gender bias. 

The translation system used is \href{https://github.com/Helsinki-NLP/Opus-MT?tab=readme-ov-file}{Opus-MT}, an open-source neural MT model. It is widely used in research, supports EN-DE translation, and is trained on real-world corpora, making it suitable for studying translation bias \citep{tiedemannOPUSMTBuildingOpen2020}. Translations are then passed through BERT, trained on a dataset I have constructed by combining and adapting several existing datasets from other researchers. The classifier is lightweight and efficient, aiming for transparent behavior and easy integration into other tools \citep{devlinBERTPretrainingDeep2019}. The final tool highlights biased parts in a simple web demo. The goal is not a perfect classifier but a working prototype that shows how such detection could be integrated into translation workflows.

The main research question is therefore: \textbf{"How can a NLP-based binary classification model detect gender bias in English-German translations?"}. 

\section{Scope}

\textbf{WRITE AFTER IMPLEMENTATION PART}
This thesis focuses only on EN-DE MT. Other language pairs are out of scope.

\section{Limitations}
\textbf{WRITE AFTER IMPLEMENTATION PART}
It becomes especially difficult to detect when sentences contain multiple subjects, indirect references, or ambiguous pronouns. For example, as \citet{barclayInvestigatingMarkersDrivers2024a} explain, the sentence “He went to see her mother” clearly implies three people, while “He went to see his mother” could refer to either two or three. These types of structures introduce ambiguity that makes annotation and evaluation much harder. Creating a dataset that captures such linguistic complexity would require significant effort and careful control of variables. One broader limitation in building datasets for complex scenarios with multiple subjects is the difficulty of isolating the influence of each gendered entity \citep{lardelliBuildingBridgesDataset2024}. When working with natural language sources, it becomes hard to tell what caused the bias in the translation. Because of this, the focus of this thesis is on simpler sentence structures with a single subject. This makes it easier to identify and explain bias patterns. It also fits the intended use case: translating business texts like job advertisements or reports, which rarely involve multiple nested clauses or ambiguous pronouns.
 

\section{Overview of Chapters}
\textbf{WRITE AFTER IMPLEMENTATION PART}
