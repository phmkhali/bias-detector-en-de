

\section{Mitigation Strategies and Current Limitations}    

Different approaches have been tested to mitigate gender bias in MT. Despite of various proposals, no single solution has emerged as definitively superior \parencite{savoldiDecadeGenderBias2025}. The following section gives an overview of these strategies, takes a closer look at one selected approach, and highlights key limitations in current research.

\subsection{Technical Mitigation Approaches}
\textcite{savoldiDecadeGenderBias2025} recently grouped the mitigation approaches suggested in the past decade of research about gender bias in MT. 

A common focus is to create new test sets or ways to measure bias in MT. Instances of these are WinoMT, MT-GenEval and GeNTE. They serve the purpose of determining the extent of gender bias present. Usually these approaches incorporate statistical evaluations or bias metrics, which can then be used for actual mitigation / detection systems. A few papers compare different MT systems or add additional types of input like a document level approach or image guided MT. It tests whether changing the system's structure and/or adding more context can reduce gender bias. However, as previously stated in subsection \ref{subsection:contextual_analysis}, many systems still struggle with coreference resolution.

Stepping inside the realm of LLMs, zero-shot detection has been deployed to automatically evaluate outputs regarding gender bias. Zero-shot in this case is the prompting of GPT models to identify gender bias in the translated text without providing specific examples nor fine tuning the models. The findings suggest that the technology is not yet ready to reliably detect biased or neutral instances without human oversight \parencite{lardelliBuildingBridgesDataset2024}. 

Furthermore, by extending the reserach of \textcite{tomalinPracticalEthicsBias2021}, \textcite{ullmannGenderBiasMachine2022} concerned herself with the pre-processing of data. The approach is to manipulate the training data \textit{before} it is fed into a ML model. This again can be divided into three strategies: (1) Downsampling, which removes data until the ratio of gendered terms is balanced, (2) Upsampling, which duplicates data to balance the ratio of gendered terms and (3), Counterfactual Augmentation by introducing opposite sentences of the under-represented terms. For example, if one corpus contains "He is a doctor", the counterfactual sentence "She is a doctor" would be added \parencite{ullmannGenderBiasMachine2022}. All of the three strategies led to substantially worse translation performances. It has been proven that the implementation of pre-processing is not feasible if the overall translation quality is significantly compromised.

Generally, all solutions operate in a narrow area, not across all languages, types of biases and systems. This again proves the sheer difficulty of finding a fix to such a multifaceted issue spanning multiple disciplines. One approach, however, has shown more promise than others in balancing bias mitigation and translation quality: model adaptation.

\subsection{Model Adaptation as a practical solution}
Model adaptation (or domain adaptation) is the fine-tuning of a MT system \textit{after} it has been trained. It was introduced as a response to the pre-processing approaches yielding subpar results \parencite{tomalinPracticalEthicsBias2021}.

This technique, as described by \textcite{tomalinPracticalEthicsBias2021}, makes use of a small gender-balanced dataset called "Tiny", containing 388 sentence pairs which were either profession-based or adjective-based. The structures of the sentences are simple and follow the following scheme: \textit{"The [PROFESSION] finished [his/her] work"} or \textit{"The [ADJECTIVE] [man/woman] finished [his/her] work"}. In order to prevent "catastrophic forgetting", a result in which the model loses its performance on the original data while learning from the new dataset, Elastic Weight Consolidation (EWC) was applied. It helps the model maintain its general translation quality while still working towards the reduction of gender bias.

This approach is particularly effective because manually removing biases from massive corpora is far too computationally intensive and unsustainable to be a reasonable solution. In contrast, model adaptation requires only a small, curated dataset, making it a more feasible and scalable solution worth further investigation.

\subsection{What counts as fair?}

One major limitation is that gender bias is not yet fully discussed in society or in language studies, so there is \textbf{no agreed standard for gender-fair language} (GFL) \parencite{lardelliBuildingBridgesDataset2024, savoldiDecadeGenderBias2025} and "fairness" heavily depends on personal views, culture, and context. Generally said, bias lies on a spectrum and changes with the chosen definition. Some argue that removing all biases is impossible \parencite{ullmannGenderBiasMachine2022}; which, for instance, leads to questions about group fairness and individual fairness. Group fairness seeks to achieve the same statistics for all groups. Individual fairness aims for similar treatment of similar people. For example, in hiring, group fairness might enforce equal hire rates for men and women. Individual fairness might enforce equal chances for two equally qualified applicants, regardless of gender. These aims can conflict, where many settings cannot satisfy both at once. 

Due to the unclear definition in academia, I have to define what I consider fair to set a clear direction. In this thesis, I focus on reducing harmful bias rather than chasing a fully unbiased state. I deem fair \textbf{a system that does not predict gender incorrectly when the correct gender is clear}. This choice guides my work.

Further challenges are ethical and linguistic considerations, which I will not further elaborate in this section. See more under %ref to limitations of my own research chapter.
