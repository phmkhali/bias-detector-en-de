





\subsection{English-German Studies}
This language pair in particular is sparsely analysed in academia. I found \textbf{four relevant papers about gender bias in EN-DE MT} that fit my inclusion criteria defined in chapter \ref{subsection:selection_criteria}. Some other sources include German among multiple target languages (e.g., \citeauthor{stanovskyEvaluatingGenderBias2019}'s foundational study), but these do not provide detailed analysis specific to German. Therefore, I do not consider them EN-DE focused sources. The following studies provide a closer look at gender bias specifically in this language pair.

\textbf{\cite{ullmannGenderBiasMachine2022}} performed a corpus-linguistic analysis of training data, meaning they studied large collections of text to identify patterns and structures related to gender bias. The dataset consisted of 17.2 million sentence pairs sourced from \href{https://commoncrawl.org/}{\textit{Common Crawl}}. They then tested different techniques to reduce gender bias in a MT system trained on that corpus. Their findings support the broader patterns discussed in this thesis: masculine forms dominate by default, gender stereotypes shape translations, and professions are translated in line with societal roles. Their key contribution lies in testing mitigation strategies. They show that fine-tuning with a small, gender-balanced dataset can reduce bias in MT outputs. 

\textbf{\citet{rescignoGenderBiasMachine2023}} evaluated gender bias in Google Translate and DeepL for EN-IT and EN-DE using the \href{https://github.com/amazon-science/machine-translation-gender-eval}{MT-GenEval} dataset. They focused on how often professions were translated with male or female forms, both with and without gender-revealing context. Without context, both systems defaulted strongly to masculine forms (over 85\%) for both languages. Contextual information generally improved alignment with reference translations, but in a few cases, context led to incorrect gender disambiguation that had not occurred without it. This suggests that contextual cues can occasionally misguide the system rather than improve performance. The authors also noted that most users are unaware of gender bias, especially if they lack fluency in the source language. Currently there is no system in place to inform them when biased translations occur.

\textbf{\citet{lardelliBuildingBridgesDataset2024}} created a \href{https://github.com/g8a9/building-bridges-gender-fair-german-mt}{Gender-Fair German Dictionary} that includes professions and common nouns for people. They tested several MT systems and evaluated translations from Wikipedia and parliamentary texts. Translations were manually annotated as masculine, feminine, gender-neutral, or gender-inclusive. They also used zero-shot detection with GPT models, where GPT tries to identify gender fairness without specific training. Results showed strong masculine bias and poor automatic detection of GFL, requiring human review and therefore proving zero-shot detection to be challenging. Unlike most research focusing on professions, this study covers a broader range of terms.

\textbf{\citet{kapplAreAllSpanish2025}} introduced \href{https://github.com/michellekappl/mt_gender_german}{WinoMTDE}, a German gender bias evaluation test set based on \citealp{stanovskyEvaluatingGenderBias2019}'s WinoMT. It contains 288 balanced German sentences with clearly gendered subjects and tests occupational stereotyping in MT from German to other gendered languages. The study found that gender bias persists due to model architecture and training data, not source language ambiguity. Major limitations of the study include the small dataset size and broad occupation categories. It also misses some bias types and faces alignment issues affecting accuracy estimates. They call for future researchers to expand the dataset, improve annotations and include diverse gender terms. This study's evaluation pipeline is the most advanced among ENâ€“DE studies for automated bias detection.

\subsection{Cross-Language Perspectives}
Gender bias in MT is not limited to English and German. Many other language pairs show similar patterns, revealing how bias is shaped by both language and the systems behind it. This section includes a few examples from other languages to show that the issue is not specific to German in order to keep the broader context in mind and avoid a narrow, language-specific perspective.

Some studies looked at \textbf{back-translation from English through gender-neutral languages} like Finnish, Indonesian, and Turkish, then back to English. They found different pronoun patterns depending on the language. This shows why it is important to study many languages to understand gender bias better. Verbs played a big role in how gender was inferred in translations. New metrics, like Adjusted Uncertainty, helped capture these details. Some translation systems showed signs of reducing bias over time \citep{barclayInvestigatingMarkersDrivers2024a}.

When translating \textbf{gender-neutral Korean into English}, MT systems often leaned toward masculine pronouns. This happened because the training data had more male examples. Some systems made technical changes that sometimes favored feminine forms, which suggests bias mitigation is possible, however ideally, translations should stay neutral or balanced \citep{choMeasuringGenderBias2019}. \textbf{Japanese and Chinese} demonstrated exceptionally low percentages of female pronouns in translations, going as low as 0.196\% for Japanese and 1.865\% for Chinese \citep{pratesAssessingGenderBias2019}.

Even when translating \textbf{between languages that both use grammatical gender}, like German and Spanish, Ukrainian, or Russian, gender bias still shows up \citep{kapplAreAllSpanish2025}. This goes against the assumption that clear grammatical cues would reduce ambiguity and help systems make better choices. Instead, the bias often stays or even gets worse, suggesting that the problem is not just about language structure but also how MT systems learn and generalize from data.

Most studies focus on English paired with another Western language, with only a few exceptions including West or East Asian languages. This adds an Anglocentric bias to the existing gender bias problem \citep{savoldiDecadeGenderBias2025}.


\section{Mitigation Strategies and Current Limitations}    

Different approaches have been tested to mitigate gender bias in MT. Despite of various proposals, no single solution has emerged as definitively superior \citep{savoldiDecadeGenderBias2025}. The following section gives an overview of these strategies, takes a closer look at one selected approach, and highlights key limitations in current research.

\subsection{Technical Mitigation Approaches}
\citet{savoldiDecadeGenderBias2025} recently grouped the mitigation approaches suggested in the past decade of research about gender bias in MT. 

A common focus is to create new test sets or ways to measure bias in MT. Instances of these are WinoMT, MT-GenEval and GeNTE. They serve the purpose of determining the extent of gender bias present. Usually these approaches incorporate statistical evaluations or bias metrics, which can then be used for actual mitigation / detection systems. A few papers compare different MT systems or add additional types of input like a document level approach or image guided MT. It tests whether changing the system's structure and/or adding more context can reduce gender bias. However, as previously stated in subsection \ref{subsection:contextual_analysis}, many systems still struggle with coreference resolution.

Stepping inside the realm of LLMs, zero-shot detection has been deployed to automatically evaluate outputs regarding gender bias. Zero-shot in this case is the prompting of GPT models to identify gender bias in the translated text without providing specific examples nor fine tuning the models. The findings suggest that the technology is not yet ready to reliably detect biased or neutral instances without human oversight \citep{lardelliBuildingBridgesDataset2024}. 

Furthermore, by extending the reserach of \citet{tomalinPracticalEthicsBias2021}, \citet{ullmannGenderBiasMachine2022} concerned herself with the pre-processing of data. The approach is to manipulate the training data \textit{before} it is fed into a ML model. This again can be divided into three strategies: (1) Downsampling, which removes data until the ratio of gendered terms is balanced, (2) Upsampling, which duplicates data to balance the ratio of gendered terms and (3), Counterfactual Augmentation by introducing opposite sentences of the under-represented terms. For example, if one corpus contains "He is a doctor", the counterfactual sentence "She is a doctor" would be added \citep{ullmannGenderBiasMachine2022}. All of the three strategies led to substantially worse translation performances. It has been proven that the implementation of pre-processing is not feasible if the overall translation quality is significantly compromised.

Generally, all solutions operate in a narrow area, not across all languages, types of biases and systems. This again proves the sheer difficulty of finding a fix to such a multifaceted issue spanning multiple disciplines. One approach, however, has shown more promise than others in balancing bias mitigation and translation quality: model adaptation.

\subsection{Model Adaptation as a practical solution}
Model adaptation (or domain adaptation) is the fine-tuning of a MT system \textit{after} it has been trained. It was introduced as a response to the pre-processing approaches yielding subpar results \citep{tomalinPracticalEthicsBias2021}.

This technique, as described by \citet{tomalinPracticalEthicsBias2021}, makes use of a small gender-balanced dataset called "Tiny", containing 388 sentence pairs which were either profession-based or adjective-based. The structures of the sentences are simple and follow the following scheme: \textit{"The [PROFESSION] finished [his/her] work"} or \textit{"The [ADJECTIVE] [man/woman] finished [his/her] work"}. In order to prevent "catastrophic forgetting", a result in which the model loses its performance on the original data while learning from the new dataset, Elastic Weight Consolidation (EWC) was applied. It helps the model maintain its general translation quality while still working towards the reduction of gender bias.

This approach is particularly effective because manually removing biases from massive corpora is far too computationally intensive and unsustainable to be a reasonable solution. In contrast, model adaptation requires only a small, curated dataset, making it a more feasible and scalable solution worth further investigation.

\subsection{What counts as fair?}

One major limitation is that gender bias is not yet fully discussed in society or in language studies, so there is \textbf{no agreed standard for gender-fair language} (GFL) \citep{lardelliBuildingBridgesDataset2024, savoldiDecadeGenderBias2025} and "fairness" heavily depends on personal views, culture, and context. Generally said, bias lies on a spectrum and changes with the chosen definition. Some argue that removing all biases is impossible \citep{ullmannGenderBiasMachine2022}; which, for instance, leads to questions about group fairness and individual fairness. Group fairness seeks to achieve the same statistics for all groups. Individual fairness aims for similar treatment of similar people. For example, in hiring, group fairness might enforce equal hire rates for men and women. Individual fairness might enforce equal chances for two equally qualified applicants, regardless of gender. These aims can conflict, where many settings cannot satisfy both at once. 

Due to the unclear definition in academia, I have to define what I consider fair to set a clear direction. In this thesis, I focus on reducing harmful bias rather than chasing a fully unbiased state. I deem fair \textbf{a system that does not predict gender incorrectly when the correct gender is clear}. This choice guides my work.

Further challenges are ethical and linguistic considerations, which I will not further elaborate in this section. See more under %ref to limitations of my own research chapter.
