\chapter{Methodology}
This chapter explains the overall approach and structure of the project. It covers how data is handled, how the model is built and trained, and how the demo application is designed.

\section{Goal of the project}
The goal is to build a gender bias detection model for real-world MT scenarios. This includes cases like translating everyday sentences or job descriptions. The focus is to flag bias at the sentence level, so users do not have to find the specific sentences causing bias themselves.

Thus, the model processes each sentence independently. If multiple sentences are inputted, bias is evaluated for each one separately. Context across sentences is not considered, as it does not reflect the intended use case. This approach is also reflected in the design of the training data, where each sentence pair is treated as a standalone instance.

\section{Workflow}
The project begins by selecting and combining datasets from previous work (see \autoref{fig:workflow}). The model building phase then follows, as shown in the purple boxes. It starts with cleaning and preparing the data, followed by extracting features for training. A pre-trained multilingual BERT model is then fine-tuned for the classification task. Its performance is measured using standard evaluation metrics. In the final step, the trained model is integrated  into the demo application.

\vspace{1cm} 
\begin{figure}[htb]
    \centering
    \scalebox{0.8}{\input{./Bilder/methodology_workflow.tex}}
    \caption{Workflow of the project.}
    \label{fig:workflow}
\end{figure}
\vspace{1cm} 

\section{Dataset Handling}
% thought process for selecting datasets from prior works
% experimental joining and testing of datasets

Since there was no ready-to-use dataset for this task and no prior work that built a similar model, I first had to define: \textbf{(1)} the number of samples required,  and \textbf{(2)} the desired content of my dataset.

\subsection{Number of Samples}
For a binary classification task of detecting gender bias using \texttt{mBERT}, general guidelines suggest between 100 and 5000 labeled samples for fine-tuning \parencite{pecherComparingSpecialisedSmall2024}, while multi-class tasks need fewer samples (around 100). However, the complex nature of gender bias often requires a larger dataset for robust detection since the number of samples depends mainly on the task type. 

\subsection{Dataset Composition}
Ideally, I wanted to make use of past EN-DE datasets to minimize manual labour. My options were \texttt{\href{https://huggingface.co/datasets/FBK-MT/mGeNTE}{mGeNTE en-de}} \parencite{savoldiMGeNTEMultilingualResource2025}, \texttt{\href{https://github.com/g8a9/building-bridges-gender-fair-german-mt}{Building Bridges Dictionary}} \parencite{lardelliBuildingBridgesDataset2024}, and \texttt{\href{https://research.google/blog/a-dataset-for-studying-gender-bias-in-translation/}{Translated Wikipedia Biographies}} \parencite{stellaDatasetStudyingGender2021}. 

During analysis of the \texttt{Translated Wikipedia Biographies} dataset, I identified issues that prevented automatic reuse. For instance, the \texttt{perceivedGender} column sometimes contained subject names instead of expected labels such as Male, Female, or Neutral, necessitating manual review. Additionally, the dataset only included neutral labels (0) since the phrases were correctly gendered. Because my other two datasets are already balanced and include enough neutrally gendered examples, I decided to exclude this dataset.

\texttt{mGeNTE} contains naturally occurring sentences with gendered entities, while \texttt{Building Bridges} focuses on German GFL entries for explicitly gendered nouns such as professions. 

\begin{table}[ht!]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|}
    \hline
    \textbf{Dataset} & \textbf{Description} & \textbf{Content} \\ \hline
    \texttt{mGeNTE en-de} \parencite{savoldiMGeNTEMultilingualResource2025} & Multilingual dataset to assess gender bias in MT. & \textasciitilde1,500 gender-ambiguous and gendered English sentences with gender-neutral and gendered German translations. \\ \hline
    \texttt{Building Bridges Dictionary} \parencite{lardelliBuildingBridgesDataset2024} & Bilingual dictionary designed to support gender-fair EN-DE translation. & \textasciitilde230 German gender-neutral and gender-inclusive singular and plural sentences with English equivalents. \\ \hline
    \end{tabularx}
    \caption{Overview of suitable EN-DE datasets based on past works.}
    \label{tab:available_datasets}
\end{table}

\subsubsection{mGeNTE en-de} The mGeNTE dataset contained the following relevant information:  

\begin{itemize}  
  \item \texttt{SET-G}: English sentences with a clearly gendered subject.  
  \item \texttt{SET-N}: English sentences with neutral or ambiguous subject gender.  
  \item \texttt{REF-G}: German translations that preserve or introduce gender.  
  \item \texttt{REF-N}: German translations that are fully gender–neutral.  
\end{itemize}  

\noindent
The bias definition used in this study classifies translations that omit the original gender as neutral since it does not rely on a male default or stereotype. While gender‑neutral translations may be imperfect, they are not considered "biased" under this framework. Initial experiments showed that including \texttt{REF-N} pairs during training overly penalized neutral outputs. Given the scarcity of neutral examples, I chose not to penalize neutral translations. 

Each original entry was split into two paired examples and labeled as follows:  
\[
\begin{aligned}
\texttt{SET\!-\!G} + \texttt{REF\!-\!G} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!G} + \texttt{REF\!-\!N} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!N} + \texttt{REF\!-\!N} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!N} + \texttt{REF\!-\!G} &\;\rightarrow\; 1 \quad(\text{biased})
\end{aligned}
\]  
This procedure yields 3,000 total instances, of which 750 are labeled biased (1) and 2,250 are labeled neutral (0). \footnote{The transformed dataset can be found in \texttt{/datasets/mgente\_final.csv}.} 

\subsubsection{Building Bridges Dictionary} 
This dataset did not contain full sentences but rather a gender-fair dictionary of nouns. While this made it a valuable resource for studying gender-fair language, I needed full sentences for my task. To address this, I used prompt engineering with Google Gemini 2.5 Flash to synthetically expand the dataset. The prompt used for generation is included in the appendix. The generated sentences can be found in the code files. I used the nouns from the original dataset to create multiple grammatically correct sentence variations, covering singular, plural, gender-neutral, and gender-inclusive forms. The dataset uses the star form (e.g., \textit{Lehrer*innen}) as its gender-inclusive format. Since the colon form (e.g., \textit{Lehrer:innen}) is also widely used, I added it manually via a script. The script duplicated all entries with stars and replaced the star with a colon to create additional variants.

This resulted in 3,381 total entries: 2,001 labeled as 0 (neutral) and 1,380 labeled as 1 (biased). \footnote{The transformed dataset can be found in \texttt{/datasets/lardelli\_final.csv}.}

However, this setup lacked genuinely neutral examples; sentences that do not involve any gendered subject at all, such as \textit{"The weather is nice"} or \textit{"How are you"}. Including such sentences is important to help the model learn that not all translations are relevant for gender bias detection and that many ordinary sentences should be classified as neutral.

\subsubsection{Tatoeba} 
Since no suitable dataset for this category was readily available, a supplementary set was created using random EN-DE sentence pairs from the \href{https://tatoeba.org/en/}{\texttt{Tatoeba}} corpus. A sample of 550 sentence pairs was selected. Manual filtering was applied to remove any pairs with incorrect or stereotypically gendered translations, as public contributions often default to male forms. The resulting subset consisted of 532 clearly neutral sentence pairs, all labeled with 0. \footnote{The transformed dataset can be found in \texttt{/datasets/tatoeba\_final.csv}.} 

\subsection{Available Data Summary}

\autoref{tab:data-summary} shows an overview of the labeled data from the three available sources. 

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Neutral (0)} & \textbf{Biased (1)} \\
\midrule
\texttt{lardelli\_final.csv} & 3381 & 2001 & 1380 \\
\texttt{mgente\_final.csv}   & 3000 & 2250 & 750  \\
\texttt{tatoeba\_final.csv}      & 532  & 532  & 0    \\
\bottomrule
\end{tabularx}
\caption{Summary of available labeled examples}
\label{tab:data-summary}
\end{table}

The number of samples selected from each dataset was determined through iterative testing. Multiple dataset variants were created by upsampling or downsampling specific groups. The documentation of this process is discussed in \autoref{subsection:training_dataset_tuning}.

\section{Data Pre-processing}
    I load and split the dataset into three parts: training (80\%), validation (10\%), and test sets (10\%). This ratio is a common split ratio to work with \parencite{bahetiTrainTestValidation2021}. It provides enough samples for the model to learn general patterns while reserving separate subsets for tuning and final evaluation. I used stratified sampling for this. That means the label distribution (biased vs. neutral) stays the same across all three sets, avoiding skewed splits. For example, if 30\% of the full dataset is biased, each split will also have 30\% biased samples. This helps the model learn from both classes equally and avoids misleading results in validation or testing.

    I did not apply advanced text cleaning such as removing punctuation, lowercasing, or stemming because I use \href{https://huggingface.co/google-bert/bert-base-multilingual-cased}{\texttt{bert-base-multilingual-cased}}. This tokenizer is designed to handle raw, unmodified text and preserves casing. The model has been trained on large corpora containing natural language in its original form \parencite{devlinBERTPretrainingDeep2019}. Altering the input, such as converting "Doctor" to "doctor", could remove distinctions the model has learned. Therefore, advanced preprocessing might reduce performance by disrupting patterns the tokenizer expects.

Further data preprocessing was not necessary since I concatenated already clean datasets and manually reviewed them.

\section{Model Initialization}
    \texttt{mBERT} with a binary classification head is used to predict whether a translation is \textit{biased} or \textit{neutral}.

    The tokenizer from the same model source encodes input pairs into token IDs and applies segment embeddings to distinguish between source and target sentences. All sequences are padded or truncated to a fixed length of 256 tokens. This length was chosen after tests with 128 tokens led to truncation warnings and content loss. With 256 tokens, most pairs remained complete while keeping memory use efficient.

    Input features and labels (0 for neutral, 1 for biased) are converted to PyTorch tensors for training. The model runs on GPU if available to speed up processing. For classification, the output vector of the [CLS] token is used as a representation of the full input.

\section{Training Pipeline}
\subsection{{Fine‑Tuning with the Trainer API.}}
    I fine-tuned the model using the Hugging Face \href{https://huggingface.co/docs/transformers/en/main_classes/trainer}{\texttt{Trainer} API}. This library takes care of most of the training process. It handles evaluation, logging, and saving model checkpoints. It also supports automatic early stopping and loading the best model at the end.

    Compared to writing a manual PyTorch training loop, \texttt{Trainer} is easier to use and requires less code. It also includes standard features like tracking metrics and scheduling learning rates. For my use case, this made it a better choice. I did not need custom loss functions or multi-GPU setups, so the added complexity of other frameworks (like PyTorch Lightning) was not necessary.

    \subsection{Layer Freezing.}
    I fine-tuned only the last two encoder layers (10 and 11), the pooler, and the classifier, about 25-30\% of the model. Later layers specialize in the fine-tuning task by focusing on features relevant to it. Freezing earlier layers keeps the general language knowledge unchanged. This reduces overfitting by limiting changes to the most task-relevant parts and speeds up training because fewer parameters are updated \parencite{nadipalliLayerWiseEvolutionRepresentations2025}. This is helpful since the task is very specific and resources are limited.

\section{Evaluation Strategy}
    I evaluated the model during training using the validation set and after training using two separate test sets. As described in \autoref{subsection:hyperparameters_explained}, I used the macro F1 score to measure model performance. The validation set helped track progress across epochs. I saved the checkpoint with the highest validation F1 score.

    However, since the joined training dataset was handcrafted and had known weaknesses, I did not rely on the validation set alone to judge final performance. To get a clearer picture of how well the model generalizes outside of that dataset, I created a separate handcrafted test set. This set includes EN-DE sentence pairs with manually added bias labels.
    
    Based on those two evaluation strategies, I adapted the fine-tuning as well as the joined training dataset. 

\subsection{Handcrafted Test Set Construction} \label{subsection:eval_dataset}
    I approached the manual test set creation from the perspective of a user testing the tool. I asked what types of input would reveal different failure cases or edge cases. I grouped the examples into categories: neutral sentences, neutral sentences with gendered roles, biased translations, and translations using German GFL. These groupings helped ensure coverage of both obvious and subtle cases.

    The set includes simple synthetic sentences I wrote myself, as well as real-world examples taken from job postings. I added the latter to reflect how the tool might be used in practice, for example, when checking a translated job ad for gender bias.

    I focused on diversity in sentence structure and content rather than label balance. Some cases were meant to test whether the model wrongly flags neutral sentences that happen to contain gendered words. Others tested its ability to recognize different GFL forms in German, such as “Lehrende” or the colon form “Lehrer:innen.” Bias labels were assigned manually based on the definition discussed in Chapter 2. The full handcrafted test set consisting of 25 labeled translation pairs is listed in the Appendix.

\subsection{Hyperparameter Selection and Tuning}
     While I tested a few standard hyperparameters, I focused more on tuning the dataset composition and the number of frozen layers. These changes had a much stronger impact on performance in my experiments. Since my training data came from a mix of external sources with varying quality, I saw more value in adjusting how the data was used rather than spending time on extensive hyperparameter tuning. The recommended defaults from prior work already performed well enough, so I treated them as a reasonable starting point.

    \paragraph{Epochs.} I trained the model for up to 8 epochs, with early stopping enabled using a patience of two epochs. This means that training stopped if the F1 score did not improve for two consecutive epochs. The setup follows the recommendation by \textcite{pecherComparingSpecialisedSmall2024}, who advise training until convergence with a maximum of 10 epochs and early stopping. In my case, the validation loss typically began to increase after 8 epochs, with no further gains in performance. Reducing the maximum to 8 epochs helped avoid overfitting and shortened training time.

    \paragraph{Batch size.} I used a batch size of 16. This is a common choice for fine-tuning on small datasets, offering a good balance between memory usage and training stability. I tested smaller batch sizes with lower learning rates, but these combinations led to worse performance and less effective training early on. Since existing literature, such as \textcite{mosbachStabilityFinetuningBERT2021}, already supports using a batch size of 16, I did not run any additional experiments with smaller values.

    \paragraph{Learning rate.} I set the learning rate to 2e-5. This value comes from the original BERT paper \parencite{devlinBERTPretrainingDeep2019} and is still widely used. I also tested 1e-5 and 3e-5, but both showed slightly worse results on the validation set. The 2e-5 setting gave the most stable and reliable performance in my runs, so I kept it for all final training runs.

    \paragraph{Optimizer and scheduler.} The \texttt{Trainer} API uses the AdamW optimizer by default. The scheduler here uses a warmup-linear schedule: the learning rate starts low and increases gradually during the first 10\% of training (warmup), then decreases linearly until the end. This helps the model learn smoothly and prevents unstable training.

    These were the values I used for the following experiments described in the \textit{Training Dataset Tuning} and \textit{Layer Freezing Tuning} sections.

\subsection{Training Dataset Tuning} \label{subsection:training_dataset_tuning}
    This section summarizes the process of tuning the training dataset composition for the classifier. The detailed documentation of each iteration is included in the appendix. Here, the focus is on the process and rationale rather than on numeric results.

    Since the F1 scores were similar across dataset versions, the main evaluation was performed my handcrafted test set of 25 sentences. This small test set does not provide a full indication of model quality, but it offers insight into practical usability. When mentioning better or worse performance, this limitation should be kept in mind.

    The dataset \texttt{mgente\_final} was considered the best source because its samples are natural sentences. All 750 biased samples from \texttt{mgente\_final} were included, along with exactly 750 neutral samples. The tuning process aimed to adjust the remaining datasets to maintain a maximum ratio of 60\% neutral to 40\% biased samples.

    Sampling was performed using the \texttt{join\_datasets.py} script, which loads the labeled datasets, samples a fixed number of biased and neutral entries per dataset using a fixed random seed (10), and combines them into a single training set. The script also checks for missing values and label integrity before saving the final CSV file. The tuning process across dataset versions, including their composition and rationale, is summarized in \autoref{tab:dataset_versions}.

    The Baseline dataset already achieved strong performance, with a test F1 score of 0.975 and 84.6\% accuracy on the handcrafted test set. However, it failed on some neutral examples such as \textit{"My mother is an engineer." / "Meine Mutter ist Ingenieurin."} (predicted biased with confidence 0.55) and on certain German GFL patterns (e.g., double naming and the colon notation).

    Tweaking the dataset composition in Datasets B through E occasionally improved specific issues. For example, Dataset E correctly classified the neutral gendered sentence that the Baseline missed. However, these improvements came at the cost of worse performance elsewhere, such as failing on some job advertisements. Overall, these adjustments did not lead to consistent practical gains on the handcrafted test set and sometimes reduced accuracy. Therefore, the Baseline dataset composition was retained for final training, as it provided the best overall balance and practical usability based on the evaluation.

\vspace{0.8em}
\begin{table}[ht]
    \centering
        \begin{tabularx}{\textwidth}{l X >{\raggedright\arraybackslash}X}
    \toprule
    \textbf{Dataset} & \textbf{Rationale} & \textbf{Sample Distribution (biased/neutral)} \\
    \midrule
    A & Initial setup using equal parts of \texttt{mgente\_final} and \texttt{lardelli\_final}, with some \texttt{tatoeba\_final} neutrals. & mgente 750 / 750, lardelli 750 / 750, tatoeba 0 / 250 \\
    B & Built on A. Increased \texttt{lardelli\_final} neutrals to better capture GFL patterns and added more \texttt{tatoeba\_final} neutrals. & mgente 750 / 750, lardelli 750 / 1000, tatoeba 0 / 400 \\
    C & Built on A and B. Reduced \texttt{lardelli\_final} biased examples to counter possible overrepresentation. & mgente 750 / 750, lardelli 400 / 750, tatoeba 0 / 250 \\
    D & Built on A and C. Further improved neutral recognition by adding more \texttt{tatoeba\_final} neutral sentences. & mgente 750 / 750, lardelli 750 / 750, tatoeba 0 / 500 \\
    E & Built on A and C. Increased \texttt{mgente\_final} neutral data to raise diversity from naturalistic examples. & mgente 750 / 1,250, lardelli 750 / 750, tatoeba 0 / 250 \\
    \bottomrule
    \end{tabularx}
    \caption{Dataset iterations with rationale and composition. Each version builds on the Baseline and previous adjustments. Format: source biased / neutral.}
    \label{tab:dataset_versions}
\end{table}


\subsection{Layer Freezing Tuning}
    All dataset tuning experiments described above were conducted with layer freezing set to $n=8$, meaning that encoder layers 0 through 7 of \texttt{mBERT} were frozen during training. As explained in \autoref{subsection:hyperparameters_explained}, earlier studies have shown that the middle layers are most semantically informative, while lower layers tend to capture syntactic information. Freezing up to layer 8 was chosen as a baseline to reduce training time while still allowing the model to adapt higher-level representations to the task.

    Since the results with $n=8$ were already promising, only two further variations were tested: $n=7$ and $n=6$. These settings freeze fewer layers, meaning more of the network remains trainable. The purpose of these tests was to evaluate whether this added flexibility improved performance without overfitting.

    \vspace{0.8em}
    \begin{table}[ht]
        \centering
        \begin{tabularx}{\linewidth}{Xcc}
        \toprule
        \textbf{Frozen Layers} & \textbf{Test F1 (weighted)} & \textbf{Handcrafted Test Set Accuracy} \\
        \midrule
        $n=6$ (layers 0--5 frozen) & 0.981 & 80.8\% \\
        $n=7$ (layers 0--6 frozen) & 0.979 & 80.8\% \\
        $n=8$ (layers 0--7 frozen) & 0.966 & 84.8\% \\
        \bottomrule
        \end{tabularx}
        \caption{Comparison of layer freezing settings}
    \end{table}

    Although freezing fewer layers yielded slightly higher F1 scores on the test set, the model trained with $n=8$ performed best on the handcrafted test sentences, which were designed to assess real-world usability. Given that the F1 differences were small and that freezing more layers leads to a simpler and more efficient model, $n=8$ was selected as the final setting.

\section{Demo Application Design}

The demo application comprises three modules: the Streamlit interface, the bias detection model with its prediction functions, and the translation component. \autoref{fig:demo_workflow} illustrates the workflow. Both input modes converge on a common prediction pipeline.

\begin{figure}[htb]
    \centering
    \scalebox{0.8}{\input{./Bilder/demo_workflow.tex}}
    \caption{Workflow of the bias detection application. Automatic translation and manual sentence pairs follow the same prediction steps.}
    \label{fig:demo_workflow}
\end{figure}

\subsubsection{Initialization} 
On launch, the application loads the fine‑tuned BERT bias detection model and its tokenizer from a local directory. These objects are cached in memory to avoid repeated loading. The model and tokenizer move to the available device, either CPU or GPU.

\subsubsection{User Interface} 
The interface has two tabs:
\begin{enumerate}
  \item \textbf{Automatic Translation.} The user inputs raw English text. The application splits the text into sentences and sends them in batches to the translation module. The module returns German translations. Each English sentence is paired with its translation before bias analysis.
  \item \textbf{Manual Pairing.} The user supplies parallel English–German sentence pairs. After splitting and pairing, the application bypasses translation and proceeds directly to bias analysis.
\end{enumerate}

\subsubsection{Bias Detection Pipeline} 
Sentence pairs are processed in batches to reduce overhead and speed up analysis. Each sentence pair is first encoded with the tokenizer of the fine‑tuned BERT model. The encoded input is then processed by the trained classifier, which produces raw bias scores. These scores are normalized into probabilities using softmax. The bias label corresponding to the highest probability is selected, and its confidence value is reported.

\subsubsection{Results Presentation} 
The application displays a table of results. Each row contains:
\begin{itemize}
  \item Original English sentence
  \item Corresponding German sentence
  \item Bias prediction with confidence score
\end{itemize}

\subsubsection{Chapter Summary}
This chapter outlined the full process of building a bias detection system, from dataset selection and preprocessing to fine-tuning a multilingual BERT model, and presenting the demo interface. While key hyperparameters were tuned to improve training stability and performance, the main focus was on practical usability. To reflect this, I crafted a second test set with edge cases that better represent real-world bias scenarios. Alongside hyperparameter tuning, most effort went into dataset construction and adjustment. This focus on data, rather than model complexity, was key to achieving stable F1 scores and making the system more useful in practice. These decisions provide a solid foundation for evaluating how well the model performs in practice, as detailed in the next chapter.