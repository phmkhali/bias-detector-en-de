\chapter{Methodology}
The goal of this project is to develop a gender bias detection model tailored for practical, real-world MT scenarios. It targets common use cases like translating everyday sentences or job descriptions, focusing on flagging biased language at the sentence level. This means the model evaluates each sentence independently, without considering context from surrounding sentences. This approach guides both the model’s design and the preparation of the training data, where each translation pair is treated as a separate example. The following sections explain the methods used to achieve this.

\section{Workflow}
The project begins by selecting and combining datasets from previous work (see \autoref{fig:workflow}). The model building phase then follows, as shown in the purple boxes. It starts with cleaning and preparing the data, followed by extracting features for training. A pre-trained \texttt{mBERT} model is then fine-tuned for the classification task. Its performance is measured using standard evaluation metrics. In the final step, the trained model is integrated  into the demo application.

\vspace{1cm} 
\begin{figure}[htb]
    \centering
    \scalebox{0.8}{\input{./Bilder/methodology_workflow.tex}}
    \caption{Workflow of the project.}
    \label{fig:workflow}
\end{figure}
\vspace{1cm} 

\section{Dataset Handling}
% thought process for selecting datasets from prior works
% experimental joining and testing of datasets

Since there was no ready-to-use dataset for this task and no prior work that built a similar model, I first had to define: \textbf{(1)} the number of samples required,  and \textbf{(2)} the desired content of my dataset.

\subsection{Number of Samples}
For a binary classification task of detecting gender bias using \texttt{mBERT}, general guidelines suggest between 100 and 5,000 labeled samples for fine-tuning \parencite{pecherComparingSpecialisedSmall2024}, while multi-class tasks need fewer samples (around 100). However, the complex nature of gender bias often requires a larger dataset for robust detection since the number of samples depends mainly on the task type. For the final dataset, 2,000 to 5,000 samples were selected to provide enough data for effective training while staying within resource limits.

\subsection{Dataset Composition}
Ideally, I wanted to make use of past EN-DE datasets to minimize manual labour. My options were \texttt{\href{https://huggingface.co/datasets/FBK-MT/mGeNTE}{mGeNTE en-de}} \parencite{savoldiMGeNTEMultilingualResource2025}, \texttt{\href{https://github.com/g8a9/building-bridges-gender-fair-german-mt}{Building Bridges Dictionary}} \parencite{lardelliBuildingBridgesDataset2024}, and \texttt{\href{https://research.google/blog/a-dataset-for-studying-gender-bias-in-translation/}{Translated Wikipedia Biographies}} \parencite{stellaDatasetStudyingGender2021}. 

During analysis of the \texttt{Translated Wikipedia Biographies} dataset, I identified issues that prevented automatic reuse. For instance, the \texttt{perceivedGender} column sometimes contained subject names instead of expected labels such as Male, Female, or Neutral, necessitating manual review. Additionally, the dataset only included neutral labels (0) since the phrases were correctly gendered. Because my other two datasets are already balanced and include enough neutrally gendered examples, I decided to exclude this dataset.

\texttt{mGeNTE} contains naturally occurring sentences with gendered entities, while \texttt{Building Bridges} focuses on German GFL entries for explicitly gendered nouns such as professions. 

\begin{table}[ht!]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|}
    \hline
    \textbf{Dataset} & \textbf{Description} & \textbf{Content} \\ \hline
    \texttt{mGeNTE en-de} \parencite{savoldiMGeNTEMultilingualResource2025} & Multilingual dataset to assess gender bias in MT. & \textasciitilde1,500 gender-ambiguous and gendered English sentences with gender-neutral and gendered German translations. \\ \hline
    \texttt{Building Bridges Dictionary} \parencite{lardelliBuildingBridgesDataset2024} & Bilingual dictionary designed to support gender-fair EN-DE translation. & \textasciitilde230 German gender-neutral and gender-inclusive singular and plural sentences with English equivalents. \\ \hline
    \end{tabularx}
    \caption{Overview of suitable EN-DE datasets based on past works.}
    \label{tab:available_datasets}
\end{table}

\subsubsection{mGeNTE en-de} The mGeNTE dataset contained the following relevant information:  

\begin{itemize}  
  \item \texttt{SET-G}: English sentences with a clearly gendered subject.  
  \item \texttt{SET-N}: English sentences with neutral or ambiguous subject gender.  
  \item \texttt{REF-G}: German translations that preserve or introduce gender.  
  \item \texttt{REF-N}: German translations that are fully gender–neutral.  
\end{itemize}  

\noindent
The bias definition used in this study classifies translations that omit the original gender as neutral since it does not rely on a male default or stereotype. While gender‑neutral translations may be imperfect, they are not considered "biased" under this framework. Initial experiments showed that including \texttt{REF-N} pairs during training overly penalized neutral outputs. Given the scarcity of neutral examples, I chose not to penalize neutral translations. 

Each original entry was split into two paired examples and labeled as follows:  
\[
\begin{aligned}
\texttt{SET\!-\!G} + \texttt{REF\!-\!G} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!G} + \texttt{REF\!-\!N} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!N} + \texttt{REF\!-\!N} &\;\rightarrow\; 0 \quad(\text{neutral})\\
\texttt{SET\!-\!N} + \texttt{REF\!-\!G} &\;\rightarrow\; 1 \quad(\text{biased})
\end{aligned}
\]  
This procedure yields 3,000 total instances, of which 750 are labeled biased (1) and 2,250 are labeled neutral (0). \footnote{The transformed dataset can be found in \texttt{/datasets/mgente\_final.csv}.} 

\subsubsection{Building Bridges Dictionary} 
This dataset did not contain full sentences but rather a gender-fair dictionary of nouns. While this made it a valuable resource for studying gender-fair language, I needed full sentences for my task. To address this, I used prompt engineering with Google Gemini 2.5 Flash to synthetically expand the dataset. The prompt used for generation is included in the appendix. The generated sentences can be found in the code files. I used the nouns from the original dataset to create multiple grammatically correct sentence variations, covering singular, plural, gender-neutral, and gender-inclusive forms. The dataset uses the star form (e.g., \textit{Lehrer*innen}) as its gender-inclusive format. Since the colon form (e.g., \textit{Lehrer:innen}) is also widely used, I added it manually via a script. The script duplicated all entries with stars and replaced the star with a colon to create additional variants.

This resulted in 3,381 total entries: 2,001 labeled as 0 (neutral) and 1,380 labeled as 1 (biased). \footnote{The transformed dataset can be found in \texttt{/datasets/lardelli\_final.csv}.}

However, this setup lacked genuinely neutral examples; sentences that do not involve any gendered subject at all, such as \textit{"The weather is nice"} or \textit{"How are you"}. Including such sentences is important to help the model learn that not all translations are relevant for gender bias detection and that many ordinary sentences should be classified as neutral.

\subsubsection{Tatoeba} 
Since no suitable dataset for this category was readily available, a supplementary set was created using random EN-DE sentence pairs from the \href{https://tatoeba.org/en/}{\texttt{Tatoeba}} corpus. A sample of 550 sentence pairs was selected. Manual filtering was applied to remove any pairs with incorrect or stereotypically gendered translations, as public contributions often default to male forms. The resulting subset consisted of 532 clearly neutral sentence pairs, all labeled with 0. \footnote{The transformed dataset can be found in \texttt{/datasets/tatoeba\_final.csv}.} 

\subsection{Available Data Summary}

\autoref{tab:data-summary} shows an overview of the labeled data from the three available sources. 

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Neutral (0)} & \textbf{Biased (1)} \\
\midrule
\texttt{lardelli\_final.csv} & 3381 & 2001 & 1380 \\
\texttt{mgente\_final.csv}   & 3000 & 2250 & 750  \\
\texttt{tatoeba\_final.csv}      & 532  & 532  & 0    \\
\bottomrule
\end{tabularx}
\caption{Summary of available labeled examples}
\label{tab:data-summary}
\end{table}

The number of samples selected from each dataset was determined through iterative testing. Multiple dataset variants were created by upsampling or downsampling specific groups. The documentation of this process is discussed in \autoref{subsection:training_dataset_tuning}.

\section{Data Pre-processing}
    The dataset is partitioned into training (80\%), validation (10\%), and test (10\%) subsets. This splitting ratio follows established practices commonly employed in ML experiments \parencite{bahetiTrainTestValidation2021}. It provides enough samples for the model to learn general patterns while reserving separate subsets for tuning and final evaluation. Stratified sampling was used to maintain consistent label distribution (biased vs. neutral) across all three sets. For example, if 30\% of the full dataset is biased, each split will also have 30\% biased samples. 

    No advanced text cleaning steps, such as punctuation removal, lowercasing, or stemming, were applied due to the use of \href{https://huggingface.co/google-bert/bert-base-multilingual-cased}{\texttt{bert-base-multilingual-cased}}. This tokenizer processes raw, unaltered text and retains case information. The model was pretrained on large corpora containing natural language in its original form \parencite{devlinBERTPretrainingDeep2019}. Modifying input text by lowercasing or removing punctuation could eliminate meaningful distinctions learned by the model. 

    No further data preprocessing was required, given that the merged datasets were pre-cleaned and verified through manual inspection.

\section{Model and Device Initialization}
% device
    \texttt{mBERT} with a binary classification head is used to predict whether a translation is \textit{biased} or \textit{neutral}.

    The tokenizer encodes input pairs into token IDs and applies segment embeddings to distinguish between source and target sentences. All sequences are padded or truncated to a fixed length of 256 tokens. This length was chosen after tests with 128 tokens led to truncation warnings and content loss. With 256 tokens, most pairs remained complete while keeping memory use efficient.

    Input features and labels (0 for neutral, 1 for biased) are converted to PyTorch tensors for training. The model runs on GPU if available to speed up processing. For classification, the output vector of the [CLS] token is used as a representation of the full input.

\subsection{Training Procedure}
    Each dataset is instantiated using a custom \texttt{BiasDataset} class, which receives a dataframe and a tokenizer. This class encodes EN-DE sentence pairs and their corresponding labels into tensors suitable for model input.

    Training hyperparameters were established through tuning, as detailed in \autoref{subsection:hyperparameter_tuning_methodology}. The training configuration is set using \texttt{TrainingArguments}, specifying evaluation and checkpoint saving strategies at each epoch.

    A \texttt{Trainer} object is then initialized with the pretrained model, training arguments, training and validation datasets, and a metric computation function. The training process is started by invoking the trainer's \texttt{train()} method, which iteratively feeds batches from the training dataset to the model, updates the model weights based on the loss, and evaluates performance on the validation set at the end of each epoch.

    The trainer automatically saves the best-performing model based on validation metrics for subsequent use in bias detection.

\section{Evaluation Strategy}
    Model evaluation was performed during training using the validation set and after training using two distinct test sets. As detailed in \autoref{subsection:hyperparameters_explained}, the macro F1 score was employed as the primary metric to assess model performance. The validation set served to monitor training progress across epochs, and the checkpoint with the highest validation F1 score was saved.

    The combined training dataset was handcrafted and had known limitations, so relying solely on the validation set was insufficient to assess final model performance. To better evaluate generalization, a separate handcrafted test set was created. This set contains EN-DE sentence pairs with manually assigned bias labels.

    Using these two evaluation strategies, both the fine-tuning process and the composition of the combined training dataset were iteratively adjusted to improve model robustness and generalization.

\subsection{Handcrafted Test Set Construction} \label{subsection:eval_dataset}
    The manual test set was developed from a user-centered perspective, focusing on identifying inputs that expose various failure and edge cases. Examples were organized into categories: neutral sentences, neutral sentences containing gendered roles, biased translations, and translations featuring German gender-fair language (GFL). 

    The test set comprises simple synthetic sentences written specifically for this purpose, as well as authentic examples extracted from job postings. The inclusion of real-world data aims to simulate practical use cases, such as evaluating translated job advertisements for gender bias.

    Emphasis was placed on diversity in sentence structure and content rather than maintaining label balance. Certain examples tested the model’s tendency to incorrectly flag neutral sentences containing gendered terms, while others assessed its capacity to detect various GFL forms in German, including terms like “Lehrende” and the colon notation “Lehrer:innen.” Bias labels were assigned manually according to the criteria established in Chapter 2. The complete handcrafted test set, containing 25 labeled translation pairs, is provided in the Appendix.


\subsection{Hyperparameter Selection and Tuning} \label{subsection:hyperparameter_tuning_methodology}
     While I tested a few standard hyperparameters, I focused more on tuning the dataset composition and the number of frozen layers. These changes had a much stronger impact on performance in my experiments. Since my training data came from a mix of external sources with varying quality, I saw more value in adjusting how the data was used rather than spending time on extensive hyperparameter tuning. The recommended defaults from prior work already performed well enough, so I treated them as a reasonable starting point.

    \paragraph{Epochs.} I trained the model for up to 8 epochs, with early stopping enabled using a patience of two epochs. This means that training stopped if the F1 score did not improve for two consecutive epochs. The setup follows the recommendation by \textcite{pecherComparingSpecialisedSmall2024}, who advise training until convergence with a maximum of 10 epochs and early stopping. In my case, the validation loss typically began to increase after 8 epochs, with no further gains in performance. Reducing the maximum to 8 epochs helped avoid overfitting and shortened training time.

    \paragraph{Batch size.} I used a batch size of 16. This is a common choice for fine-tuning on small datasets, offering a good balance between memory usage and training stability. I tested smaller batch sizes with lower learning rates, but these combinations led to worse performance and less effective training early on. Since existing literature, such as \textcite{mosbachStabilityFinetuningBERT2021}, already supports using a batch size of 16, I did not run any additional experiments with smaller values.

    \paragraph{Learning rate.} I set the learning rate to 2e-5. This value comes from the original BERT paper \parencite{devlinBERTPretrainingDeep2019} and is still widely used. I also tested 1e-5 and 3e-5, but both showed slightly worse results on the validation set. The 2e-5 setting gave the most stable and reliable performance in my runs, so I kept it for all final training runs.

    \paragraph{Optimizer and scheduler.} The \texttt{Trainer} API uses the AdamW optimizer by default. The scheduler here uses a warmup-linear schedule: the learning rate starts low and increases gradually during the first 10\% of training (warmup), then decreases linearly until the end. This helps the model learn smoothly and prevents unstable training.

    These were the values I used for the following experiments described in the \textit{Training Dataset Tuning} and \textit{Layer Freezing Tuning} sections.

\subsection{Training Dataset Tuning} \label{subsection:training_dataset_tuning}
    This section summarizes the process of tuning the training dataset composition for the classifier. The detailed documentation of each iteration is included in the appendix. Here, the focus is on the process and rationale rather than on numeric results.

    Since the F1 scores were similar across dataset versions, the main evaluation was performed my handcrafted test set of 25 sentences. This small test set does not provide a full indication of model quality, but it offers insight into practical usability. When mentioning better or worse performance, this limitation should be kept in mind.

    The dataset \texttt{mgente\_final} was considered the best source because its samples are natural sentences. All 750 biased samples from \texttt{mgente\_final} were included, along with exactly 750 neutral samples. The tuning process aimed to adjust the remaining datasets to maintain a maximum ratio of 60\% neutral to 40\% biased samples.

    Sampling was performed using the \texttt{join\_datasets.py} script, which loads the labeled datasets, samples a fixed number of biased and neutral entries per dataset using a fixed random seed (10), and combines them into a single training set. The script also checks for missing values and label integrity before saving the final CSV file. The tuning process across dataset versions, including their composition and rationale, is summarized in \autoref{tab:dataset_versions}.

    The Baseline dataset already achieved strong performance, with a test F1 score of 0.975 and 84.6\% accuracy on the handcrafted test set. However, it failed on some neutral examples such as \textit{"My mother is an engineer." / "Meine Mutter ist Ingenieurin."} (predicted biased with confidence 0.55) and on certain German GFL patterns (e.g., double naming and the colon notation).

    Tweaking the dataset composition in Datasets B through E occasionally improved specific issues. For example, Dataset E correctly classified the neutral gendered sentence that the Baseline missed. However, these improvements came at the cost of worse performance elsewhere, such as failing on some job advertisements. Overall, these adjustments did not lead to consistent practical gains on the handcrafted test set and sometimes reduced accuracy. Therefore, the Baseline dataset composition was retained for final training, as it provided the best overall balance and practical usability based on the evaluation.

\vspace{0.8em}
\begin{table}[ht]
    \centering
        \begin{tabularx}{\textwidth}{l X >{\raggedright\arraybackslash}X}
    \toprule
    \textbf{Dataset} & \textbf{Rationale} & \textbf{Sample Distribution (biased/neutral)} \\
    \midrule
    A & Initial setup using equal parts of \texttt{mgente\_final} and \texttt{lardelli\_final}, with some \texttt{tatoeba\_final} neutrals. & mgente 750 / 750, lardelli 750 / 750, tatoeba 0 / 250 \\
    B & Built on A. Increased \texttt{lardelli\_final} neutrals to better capture GFL patterns and added more \texttt{tatoeba\_final} neutrals. & mgente 750 / 750, lardelli 750 / 1000, tatoeba 0 / 400 \\
    C & Built on A and B. Reduced \texttt{lardelli\_final} biased examples to counter possible overrepresentation. & mgente 750 / 750, lardelli 400 / 750, tatoeba 0 / 250 \\
    D & Built on A and C. Further improved neutral recognition by adding more \texttt{tatoeba\_final} neutral sentences. & mgente 750 / 750, lardelli 750 / 750, tatoeba 0 / 500 \\
    E & Built on A and C. Increased \texttt{mgente\_final} neutral data to raise diversity from naturalistic examples. & mgente 750 / 1,250, lardelli 750 / 750, tatoeba 0 / 250 \\
    \bottomrule
    \end{tabularx}
    \caption{Dataset iterations with rationale and composition. Each version builds on the Baseline and previous adjustments. Format: source biased / neutral.}
    \label{tab:dataset_versions}
\end{table}


\subsection{Layer Freezing Tuning}
    All dataset tuning experiments described above were conducted with layer freezing set to $n=8$, meaning that encoder layers 0 through 7 of \texttt{mBERT} were frozen during training. As explained in \autoref{subsection:hyperparameters_explained}, earlier studies have shown that the middle layers are most semantically informative, while lower layers tend to capture syntactic information. Freezing up to layer 8 was chosen as a baseline to reduce training time while still allowing the model to adapt higher-level representations to the task.

    Since the results with $n=8$ were already promising, only two further variations were tested: $n=7$ and $n=6$. These settings freeze fewer layers, meaning more of the network remains trainable. The purpose of these tests was to evaluate whether this added flexibility improved performance without overfitting.

    \vspace{0.8em}
    \begin{table}[ht]
        \centering
        \begin{tabularx}{\linewidth}{Xcc}
        \toprule
        \textbf{Frozen Layers} & \textbf{Test F1 (weighted)} & \textbf{Handcrafted Test Set Accuracy} \\
        \midrule
        $n=6$ (layers 0--5 frozen) & 0.981 & 80.8\% \\
        $n=7$ (layers 0--6 frozen) & 0.979 & 80.8\% \\
        $n=8$ (layers 0--7 frozen) & 0.966 & 84.8\% \\
        \bottomrule
        \end{tabularx}
        \caption{Comparison of layer freezing settings}
    \end{table}

    Although freezing fewer layers yielded slightly higher F1 scores on the test set, the model trained with $n=8$ performed best on the handcrafted test sentences, which were designed to assess real-world usability. Given that the F1 differences were small and that freezing more layers leads to a simpler and more efficient model, $n=8$ was selected as the final setting.

\section{Demo Application Design}

The demo application comprises three modules: the Streamlit interface, the bias detection model with its prediction functions, and the translation component. \autoref{fig:demo_workflow} illustrates the workflow. Both input modes converge on a common prediction pipeline.

\begin{figure}[htb]
    \centering
    \scalebox{0.8}{\input{./Bilder/demo_workflow.tex}}
    \caption[Workflow of the bias detection application]{Workflow of the bias detection application. Automatic translation and manual sentence pairs follow the same prediction steps.}
    \label{fig:demo_workflow}
\end{figure}

\subsubsection{Initialization} 
On launch, the application loads the fine‑tuned BERT bias detection model and its tokenizer from a local directory. These objects are cached in memory to avoid repeated loading. The model and tokenizer move to the available device, either CPU or GPU.

\subsubsection{User Interface} 
The interface has two tabs:
\begin{enumerate}
  \item \textbf{Automatic Translation.} The user inputs raw English text. The application splits the text into sentences and sends them in batches to the translation module. The module returns German translations. Each English sentence is paired with its translation before bias analysis.
  \item \textbf{Manual Pairing.} The user supplies parallel English–German sentence pairs. After splitting and pairing, the application bypasses translation and proceeds directly to bias analysis.
\end{enumerate}

\subsubsection{Bias Detection Pipeline} 
Sentence pairs are processed in batches to reduce overhead and speed up analysis. Each sentence pair is first encoded with the tokenizer of the fine‑tuned BERT model. The encoded input is then processed by the trained classifier, which produces raw bias scores. These scores are normalized into probabilities using softmax. The bias label corresponding to the highest probability is selected, and its confidence value is reported.

\subsubsection{Results Presentation} 
The application displays a table of results. Each row contains:
\begin{itemize}
  \item Original English sentence
  \item Corresponding German sentence
  \item Bias prediction with confidence score
\end{itemize}

\subsubsection{Chapter Summary}
This chapter outlined the full process of building a bias detection system, from dataset selection and preprocessing to fine-tuning a multilingual BERT model, and presenting the demo interface. While key hyperparameters were tuned to improve training stability and performance, the main focus was on practical usability. To reflect this, I crafted a second test set with edge cases that better represent real-world bias scenarios. Alongside hyperparameter tuning, most effort went into dataset construction and adjustment. This focus on data, rather than model complexity, was key to achieving stable F1 scores and making the system more useful in practice. These decisions provide a solid foundation for evaluating how well the model performs in practice, as detailed in the next chapter.