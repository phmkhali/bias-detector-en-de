\chapter{Conclusion and Discussion}
   Without a doubt, MT systems and their accessibility drastically improve our ability to communicate with one another. While modern NMT provides semantically accurate translations for high-resource languages, they often introduce gender bias when translating between languages with and without grammatical gender. This thesis attempted to create an application to detect gender bias in EN-DE translations in real time. The findings show that an mBERT model fine-tuned for this binary classification task can detect gender bias with an accuracy of 96.6\% on a held-out test set, which comes from a train–test split of a combined dataset built from existing studies. On a separate handcrafted dataset designed to test edge cases, the model reaches 84.6\% accuracy. The model shows strong performance in identifying generic masculines and the assignment of stereotypical roles, which make up the majority of gender biases found in MT systems \parencite{lardelliBuildingBridgesDataset2024,stanovskyEvaluatingGenderBias2019,pratesAssessingGenderBias2019}. Despite its high accuracy, there are recurring error patterns that affect the model’s reliability. The error analysis revealed four sources of occasional incorrect predictions: (1) misclassifying German GFL forms as biased, (2) failing to detect semantically gendered words as unbiased, (3) failing to detect bias in political and government terms, (4) showing sensitivity to punctuation and capitalization, as well as (5) struggling with sentences that contain both neutral and gendered subjects.

   Taking everything into account, detecting gender bias in MT remains a challenging task. In morphologically rich languages like German, the subtle linguistic patterns are particularly difficult for a model to learn. Nonetheless, the model's strong performance in practical scenarios, including the recognition of generic masculines and stereotypical role assignments in job postings, demonstrates its efficacy in flagging potential bias for users. This establishes the model as a crucial intermediary layer: it raises awareness by preventing bias from going unnoticed and contributes to mitigating the representational harm that biased translations can reinforce.

\section{Limitations of this work}
   This work is primarily limited by its design choices and the resources that were available. The bias detector analyzes sentences in isolation, which allows users to quickly see the flagged sentences. This design simplifies processing but also means that bias depending on broader context can be missed. For instance, if a text introduces “the doctor is Mr. Smith” and later uses a male form for “the doctor,” this is not actually biased, but the system cannot account for it. Detecting such cases would require word-level analysis and annotated data, which were not available. Performance is also limited by the small size of high-quality datasets. To mitigate this, synthetic data and a handcrafted evaluation set were used, but this introduces the risk that subjective choices propagate into the model. Similarly, the system assumes that translations, whether generated by OpusMT or entered manually, are correct. Mistakes in translation can therefore cause misleading predictions, since translation quality was not taken into account in this work. These constraints are compounded by the model’s sensitivity to sentence-level features. Classification can be influenced by capitalization, punctuation, or multiple subjects in a sentence. Moreover, the lack of interpretability measures prevents the model from explaining why it flagged a sentence as biased, which limits understanding of its predictions and potential corrective actions. Overall, these choices result in a sentence-level tool capable of detecting explicit gender bias, but they limit the system’s ability to handle context, translation errors, and more subtle linguistic phenomena.

\section{Outlook}
   To address these limitations, adding more samples and incorporating natural, word-level labeled data would give the model more diverse examples, allowing it to better capture different patterns of gender bias and handle context-dependent cases. Expanding coverage to additional domains, such as adjective-based biases, would make the system more robust across different types of texts. Moreover, moving beyond binary classification toward token-level analysis could enable the model to identify specific words or phrases responsible for bias and classify them more precisely, for instance as “fair” or “generic masculine.” These improvements would not only enhance the accuracy of bias detection but also provide users with clearer guidance on how translations may perpetuate gendered assumptions.

   The interdisciplinary nature of this task means that progress in ML must be accompanied by advances in social and linguistic research. Understanding these dimensions of GFL helps create more reliable datasets and better ways to measure it. At the same time, exploring alternative model architectures, such as LLM with prompt engineering, could reveal new approaches to bias detection. Ultimately, the long-term goal is for MT systems to handle gendered languages more accurately on their own, reducing the need for intermediary tools. Until that point, applications like the one presented here make bias visible and support more equitable translation practices.
